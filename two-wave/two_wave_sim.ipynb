{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Two-wave RPS Algorithm",
   "id": "3821486d2362d731"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T05:38:40.836756Z",
     "start_time": "2025-06-21T05:38:40.823864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from rashomon.hasse import enumerate_policies\n",
    "from rashomon.aggregate import RAggregate\n",
    "from first_wave import compute_boundary_probs, allocate_wave, assign_first_wave_treatments\n",
    "from data_gen import get_beta_underlying_causal, generate_outcomes"
   ],
   "id": "5242302164f6663e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. First-wave allocation",
   "id": "df6535a12f16459b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T05:38:41.187745Z",
     "start_time": "2025-06-21T05:38:41.180765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# get lattice\n",
    "M = 4\n",
    "R = 3\n",
    "\n",
    "R_vec = np.full(M, R) if np.isscalar(R) else np.array(R) # allow for heterogeneity in levels\n",
    "assert R_vec.shape == (M,)\n",
    "policies = enumerate_policies(M, R)\n",
    "\n",
    "K = len(policies)\n",
    "print(f\"Found K = {K} policies (each policy is an {M}-tuple).\")\n",
    "H = 5  # sparsity parameter used inside compute_boundary_probs TODO choice\n",
    "n1 = 500  # total first‐wave sample size"
   ],
   "id": "193da5cfc81258b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found K = 81 policies (each policy is an 4-tuple).\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Compute first‐wave allocation**: We need R_i for each feature (here R_i = R for i=0,…,M-1), then we call `compute_boundary_probs` -> `allocate_first_wave`. We get `n1_alloc`: an array of length K summing to n1.",
   "id": "c7c2ae3f8cef0d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T05:38:41.484071Z",
     "start_time": "2025-06-21T05:38:41.478384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "boundary_probs = compute_boundary_probs(policies, R, H)\n",
    "n1_alloc = allocate_wave(boundary_probs, n1)\n",
    "print(f\"First‐wave allocation sums to {int(n1_alloc.sum())} (should be {n1}).\")"
   ],
   "id": "6dcc0a9d2d342815",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First‐wave allocation sums to 500 (should be 500).\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Simulating first-wave outcomes",
   "id": "db2cf32df91f077f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We generate a np.array `beta` of true effects for each node. We pass our lattice `policies`, `M` and `R`, and then specify a `kind` of underlying causal model.\n",
    "\n",
    "There are a range of options, all of which are continuous and non-trivial: they exhibit locally correlated effects and avoid brittle cancellations in effects. The options range from simple (polynomial, gaussian, basic interaction) to complex (radial basis function, mimic of a simple neural-net-like function)"
   ],
   "id": "783d2bf35aa2605f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T05:38:41.894028Z",
     "start_time": "2025-06-21T05:38:41.891355Z"
    }
   },
   "cell_type": "code",
   "source": "beta = get_beta_underlying_causal(policies, M, R, kind=\"gauss_sin\")",
   "id": "37f3b09738cfdc4e",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T05:38:42.028465Z",
     "start_time": "2025-06-21T05:38:42.026602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Not in use: different distribution for each true pool from a random 'true' partition sigma_true. Not used in this simulation due to our specifications on the underlying causal model (e.g. continuous, locally correlated effects, etc). Also needs changes on how it constructs a true partition.\n",
    "\n",
    "# partition_seed = 123\n",
    "# sigma_true, pi_pools_true, pi_policies_true = generate_true_partition(policies, R,random_seed=partition_seed)\n",
    "# beta = get_beta_piecewise(policies, sigma_true, pi_pools_true, pi_policies_true, 0.5, 1, 10)"
   ],
   "id": "c0124dca96a59a16",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Get outcomes**: we now track the first-wave assignment and generate the outcomes with additional noise",
   "id": "96414d2d89e71d3c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T05:38:42.345539Z",
     "start_time": "2025-06-21T05:38:42.341310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# now build first-wave assignment vector D\n",
    "policies = np.array(enumerate_policies(M, R))  # (K, M)\n",
    "D1 = assign_first_wave_treatments(n1_alloc)  # (N1, M)\n",
    "print(\"D1 shape:\", D1.shape)\n",
    "N1 = D1.shape[0]\n",
    "print(\"Length of D1:\", N1)  # should equal sum n1_alloc == n1"
   ],
   "id": "4cb7d7c5a1785390",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1 shape: (500,)\n",
      "Length of D1: 500\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T05:38:42.515129Z",
     "start_time": "2025-06-21T05:38:42.505903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# generate outcomes y1\n",
    "sigma_noise = 5\n",
    "outcome_seed = 53\n",
    "y1 = generate_outcomes(D=D1, beta=beta, sigma_noise=sigma_noise, random_seed=outcome_seed)\n",
    "print(\"Overall mean outcome:\", np.mean(y1))\n",
    "print(\"Overall std outcome:\", np.std(y1))"
   ],
   "id": "a9a34a6d63a9e449",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall mean outcome: -0.06186879481060036\n",
      "Overall std outcome: 5.345327463311591\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. RPS for profiles with data",
   "id": "d65ec763ab104f39"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We now search for the optimal theta as given by a normalized loss and chosen epsilon. Need to already specify H and the regularization parameter.",
   "id": "30fbf3ae778f228f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T05:38:42.982566Z",
     "start_time": "2025-06-21T05:38:42.979807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lambda_r = 0.3\n",
    "eps = 0.05 # chosen tolerance"
   ],
   "id": "bf3efc3328c7bf10",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T05:38:43.126862Z",
     "start_time": "2025-06-21T05:38:43.123948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from rashomon.hasse import enumerate_policies, enumerate_profiles, policy_to_profile\n",
    "from rashomon.aggregate import (\n",
    "    RAggregate_profile,\n",
    "    subset_data,\n",
    "    find_profile_lower_bound\n",
    ")\n",
    "from rashomon import loss"
   ],
   "id": "6dbeb16e2bb5cc0f",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T05:38:43.363599Z",
     "start_time": "2025-06-21T05:38:43.360346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 1: Enumerate profiles and map policies to each\n",
    "profiles, profile_map = enumerate_profiles(M)\n",
    "all_policies = enumerate_policies(M, R_vec)\n",
    "\n",
    "profile_to_policies = {}\n",
    "profile_to_indices = {}\n",
    "for i, pol in enumerate(all_policies):\n",
    "    pid = profile_map[policy_to_profile(pol)]\n",
    "    profile_to_policies.setdefault(pid, []).append(pol)\n",
    "    profile_to_indices.setdefault(pid, []).append(i)"
   ],
   "id": "d30d48d97739eb63",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We now filter for the profiles just with any data.",
   "id": "b79afcee8eee6bfa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T05:38:44.052376Z",
     "start_time": "2025-06-21T05:38:44.032101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 2: Filter profiles with data and compute normalized lower-bound losses\n",
    "valid_pids = []\n",
    "lb_k = []  # normalized lower-bound loss for each valid profile\n",
    "\n",
    "for pid, profile in enumerate(profiles):\n",
    "    Dk, yk = subset_data(D1, y1, profile_to_indices[pid])\n",
    "    if Dk is None:\n",
    "        continue\n",
    "    mask = np.array(profile, dtype=bool)\n",
    "    reduced_policies = [tuple(np.array(p)[mask]) for p in profile_to_policies[pid]]\n",
    "    pm = loss.compute_policy_means(Dk, yk, len(reduced_policies))\n",
    "    raw_lb = find_profile_lower_bound(Dk, yk, pm)\n",
    "    lb_k.append(raw_lb / N1)\n",
    "    valid_pids.append(pid)\n",
    "\n",
    "lb_k = np.array(lb_k)                   # array of normalized lower bounds\n",
    "best_loss = lb_k.min()                 # best profile loss\n",
    "total_lb = lb_k.sum()\n",
    "theta_global = total_lb * (1 + eps) # Theta is in reference to total loss here, not a relative value\n",
    "print(f\"best_loss = {best_loss:.5f}\")\n",
    "print(f\"theta_global = {theta_global:.5f}\")"
   ],
   "id": "8e89eb7377549fa2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_loss = 0.09952\n",
      "theta_global = 25.23455\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We now construct the RPS for each profile with data from our first allocation.",
   "id": "d090de1185f966ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T05:38:44.798976Z",
     "start_time": "2025-06-21T05:38:44.752857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "R_profiles = []\n",
    "loss_args = []\n",
    "\n",
    "for i, pid in enumerate(valid_pids):\n",
    "    profile_mask = np.array(profiles[pid], dtype=bool)\n",
    "    M_k = profile_mask.sum()\n",
    "    R_k = R_vec[profile_mask]\n",
    "\n",
    "    Dk, yk = subset_data(D1, y1, profile_to_indices[pid])\n",
    "    reduced_policies = [tuple(np.array(p)[profile_mask]) for p in profile_to_policies[pid]]\n",
    "    pm = loss.compute_policy_means(Dk, yk, len(reduced_policies))\n",
    "\n",
    "    theta_k = max(0.0, theta_global - (total_lb - lb_k[i]))\n",
    "\n",
    "    print(f\"Calling RAggregate_profile on profile {pid}, M_k={M_k}, len(policies)={len(reduced_policies)}, theta_k={theta_k:.5f}\")\n",
    "    print(f\": lower_bound: {lb_k[i]:.5f}, theta_k: {theta_k:.5f}\")\n",
    "\n",
    "    rp = RAggregate_profile(\n",
    "        M=M_k,\n",
    "        R=R_k,\n",
    "        H=H,\n",
    "        D=Dk,\n",
    "        y=yk,\n",
    "        theta=theta_k,\n",
    "        profile=tuple(profiles[pid]),\n",
    "        reg=lambda_r,\n",
    "        policies=reduced_policies,\n",
    "        policy_means=pm,\n",
    "        normalize=N1\n",
    "    )\n",
    "\n",
    "    print(f\": RPS size for profile {pid}: {len(rp)}\")\n",
    "    if len(rp) > 0:\n",
    "        R_profiles.append(rp)\n",
    "        loss_args.append((Dk, yk, reduced_policies, pm))"
   ],
   "id": "d864d078a9c1d824",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling RAggregate_profile on profile 1, M_k=1, len(policies)=2, theta_k=1.85380\n",
      ": lower_bound: 0.65216, theta_k: 1.85380\n",
      ": RPS size for profile 1: 2\n",
      "Calling RAggregate_profile on profile 2, M_k=1, len(policies)=2, theta_k=1.55939\n",
      ": lower_bound: 0.35774, theta_k: 1.55939\n",
      ": RPS size for profile 2: 2\n",
      "Calling RAggregate_profile on profile 3, M_k=2, len(policies)=4, theta_k=2.28029\n",
      ": lower_bound: 1.07864, theta_k: 2.28029\n",
      ": RPS size for profile 3: 4\n",
      "Calling RAggregate_profile on profile 4, M_k=1, len(policies)=2, theta_k=1.30117\n",
      ": lower_bound: 0.09952, theta_k: 1.30117\n",
      ": RPS size for profile 4: 2\n",
      "Calling RAggregate_profile on profile 5, M_k=2, len(policies)=4, theta_k=2.28977\n",
      ": lower_bound: 1.08812, theta_k: 2.28977\n",
      ": RPS size for profile 5: 4\n",
      "Calling RAggregate_profile on profile 6, M_k=2, len(policies)=4, theta_k=1.79353\n",
      ": lower_bound: 0.59188, theta_k: 1.79353\n",
      ": RPS size for profile 6: 4\n",
      "Calling RAggregate_profile on profile 7, M_k=3, len(policies)=8, theta_k=3.94933\n",
      ": lower_bound: 2.74769, theta_k: 3.94933\n",
      ": RPS size for profile 7: 0\n",
      "Calling RAggregate_profile on profile 8, M_k=1, len(policies)=2, theta_k=1.40522\n",
      ": lower_bound: 0.20357, theta_k: 1.40522\n",
      ": RPS size for profile 8: 2\n",
      "Calling RAggregate_profile on profile 9, M_k=2, len(policies)=4, theta_k=2.09451\n",
      ": lower_bound: 0.89287, theta_k: 2.09451\n",
      ": RPS size for profile 9: 4\n",
      "Calling RAggregate_profile on profile 10, M_k=2, len(policies)=4, theta_k=2.19088\n",
      ": lower_bound: 0.98924, theta_k: 2.19088\n",
      ": RPS size for profile 10: 4\n",
      "Calling RAggregate_profile on profile 11, M_k=3, len(policies)=8, theta_k=2.85936\n",
      ": lower_bound: 1.65772, theta_k: 2.85936\n",
      ": RPS size for profile 11: 0\n",
      "Calling RAggregate_profile on profile 12, M_k=2, len(policies)=4, theta_k=2.26666\n",
      ": lower_bound: 1.06502, theta_k: 2.26666\n",
      ": RPS size for profile 12: 4\n",
      "Calling RAggregate_profile on profile 13, M_k=3, len(policies)=8, theta_k=2.97691\n",
      ": lower_bound: 1.77527, theta_k: 2.97691\n",
      ": RPS size for profile 13: 0\n",
      "Calling RAggregate_profile on profile 14, M_k=3, len(policies)=8, theta_k=3.89039\n",
      ": lower_bound: 2.68874, theta_k: 3.89039\n",
      ": RPS size for profile 14: 0\n",
      "Calling RAggregate_profile on profile 15, M_k=4, len(policies)=16, theta_k=9.34637\n",
      ": lower_bound: 8.14473, theta_k: 9.34637\n",
      ": RPS size for profile 15: 0\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T05:38:45.116796Z",
     "start_time": "2025-06-21T05:38:45.106879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compute loss only for nonempty profile RPSs\n",
    "for rp, (Dk, yk, policies_k, pm_k) in zip(R_profiles, loss_args):\n",
    "    rp.calculate_loss(Dk, yk, policies_k, pm_k, lambda_r, normalize=N1)"
   ],
   "id": "3fa23bf50b57be30",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Construct the full RPS",
   "id": "39a3c974e7fa947e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We demonstrate the creation of the full RPS from the profile-specific partitions.",
   "id": "bd9c8723d19ab924"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T05:38:47.242291Z",
     "start_time": "2025-06-21T05:38:47.235125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 1: Data checks\n",
    "D = np.asarray(D1)\n",
    "y = np.asarray(y1)\n",
    "if D.ndim != 1:\n",
    "    raise ValueError(f\"D should be 1D (policy indices), got shape {D.shape}\")\n",
    "if y.ndim != 1:\n",
    "    y = y.ravel()\n",
    "N = len(D)\n",
    "if len(y) != N:\n",
    "    raise ValueError(f\"y and D must have same length: got {len(y)} and {N}\")\n",
    "\n",
    "\n",
    "# Enumerate all policies and profiles\n",
    "profiles, profile_map = enumerate_profiles(M)\n",
    "all_policies = enumerate_policies(M, R_vec)\n",
    "policy_to_pid = {tuple(pol): profile_map[policy_to_profile(pol)] for pol in all_policies}\n",
    "policy_to_indices = {pid: [] for pid in range(len(profiles))}\n",
    "for i, pol in enumerate(all_policies):\n",
    "    pid = policy_to_pid[tuple(pol)]\n",
    "    policy_to_indices[pid].append(i)\n"
   ],
   "id": "ce6fb0f1bee546f2",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T05:38:47.538575Z",
     "start_time": "2025-06-21T05:38:47.523639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 2: Identify active profiles and compute lower bounds\n",
    "valid_pids = []\n",
    "lb_k = []\n",
    "for pid, profile in enumerate(profiles):\n",
    "    indices = policy_to_indices[pid]\n",
    "    idx_mask = np.isin(D, indices)\n",
    "    if np.sum(idx_mask) > 0:\n",
    "        Dk_policyidx = D[idx_mask]         # Global policy indices for this profile\n",
    "        yk = y[idx_mask]\n",
    "        profile_mask = np.array(profile, dtype=bool)\n",
    "        # Map to reduced policies (profile-local tuples)\n",
    "        policies_k = [tuple(np.array(all_policies[pol_idx])[profile_mask]) for pol_idx in Dk_policyidx]\n",
    "        policies_k_unique = list(sorted(set(policies_k)))\n",
    "        # Build tuple->local index mapping for this profile\n",
    "        tuple_to_local_idx = {p: i for i, p in enumerate(policies_k_unique)}\n",
    "        Dk_local = np.array([tuple_to_local_idx[p] for p in policies_k])\n",
    "        pm = loss.compute_policy_means(Dk_local, yk, len(policies_k_unique))\n",
    "        raw_lb = find_profile_lower_bound(Dk_local, yk.reshape(-1, 1), pm)\n",
    "        lb_k.append(raw_lb / N)\n",
    "        valid_pids.append(pid)\n",
    "\n",
    "    else:\n",
    "        lb_k.append(0.0)\n",
    "\n",
    "lb_k_arr = np.array(lb_k)\n",
    "total_lb = lb_k_arr.sum()\n",
    "theta_global = total_lb * (1 + eps)\n",
    "print(f\"Observed profiles: {valid_pids}\")\n",
    "print(f\"Threshold: {theta_global:.5f}\")\n",
    "print(f\"Per-profile lower bounds: {lb_k_arr}\")"
   ],
   "id": "7be58726134ef2ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed profiles: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Threshold: 25.23455\n",
      "Per-profile lower bounds: [0.         0.65215607 0.35774498 1.07864121 0.09952396 1.08812371\n",
      " 0.5918832  2.74768663 0.2035705  0.89286711 0.989239   1.65771603\n",
      " 1.06501584 1.77526929 2.68874073 8.14472582]\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T05:38:47.910897Z",
     "start_time": "2025-06-21T05:38:47.862377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 3: Build per-profile RashomonSets (always using local indices!)\n",
    "R_profiles = []\n",
    "for i, pid in enumerate(valid_pids):\n",
    "    indices = policy_to_indices[pid]\n",
    "    idx_mask = np.isin(D, indices)\n",
    "    Dk_policyidx = D[idx_mask]\n",
    "    yk = y[idx_mask]\n",
    "    profile_mask = np.array(profiles[pid], dtype=bool)\n",
    "    M_k = profile_mask.sum()\n",
    "    R_k = R_vec[profile_mask]\n",
    "    policies_k = [tuple(np.array(all_policies[pol_idx])[profile_mask]) for pol_idx in Dk_policyidx]\n",
    "    policies_k_unique = list(sorted(set(policies_k)))\n",
    "    tuple_to_local_idx = {p: j for j, p in enumerate(policies_k_unique)}\n",
    "    Dk_local = np.array([tuple_to_local_idx[p] for p in policies_k])\n",
    "    pm = loss.compute_policy_means(Dk_local, yk, len(policies_k_unique))\n",
    "    theta_k = max(0.0, theta_global - (total_lb - lb_k[pid]))\n",
    "    print(f\"Profile {pid}: M_k={M_k}, #policies={len(policies_k_unique)}, theta_k={theta_k:.5f}\")\n",
    "    rp = RAggregate_profile(\n",
    "        M=M_k,\n",
    "        R=R_k,\n",
    "        H=H,\n",
    "        D=Dk_local.reshape(-1, 1),          # 1D array of local indices\n",
    "        y=yk.reshape(-1, 1),                # 1D array of outcomes\n",
    "        theta=theta_k,\n",
    "        profile=tuple(profiles[pid]),\n",
    "        reg=lambda_r,\n",
    "        policies=policies_k_unique,\n",
    "        policy_means=pm,\n",
    "        normalize=N\n",
    "    )\n",
    "    print(f\": RPS size: {len(rp)}\")\n",
    "    R_profiles.append(rp)"
   ],
   "id": "de43f894dc57d2fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile 1: M_k=1, #policies=1, theta_k=1.85380\n",
      ": RPS size: 2\n",
      "Profile 2: M_k=1, #policies=1, theta_k=1.55939\n",
      ": RPS size: 2\n",
      "Profile 3: M_k=2, #policies=3, theta_k=2.28029\n",
      ": RPS size: 4\n",
      "Profile 4: M_k=1, #policies=1, theta_k=1.30117\n",
      ": RPS size: 2\n",
      "Profile 5: M_k=2, #policies=3, theta_k=2.28977\n",
      ": RPS size: 4\n",
      "Profile 6: M_k=2, #policies=3, theta_k=1.79353\n",
      ": RPS size: 4\n",
      "Profile 7: M_k=3, #policies=7, theta_k=3.94933\n",
      ": RPS size: 0\n",
      "Profile 8: M_k=1, #policies=1, theta_k=1.40522\n",
      ": RPS size: 2\n",
      "Profile 9: M_k=2, #policies=3, theta_k=2.09451\n",
      ": RPS size: 4\n",
      "Profile 10: M_k=2, #policies=3, theta_k=2.19088\n",
      ": RPS size: 4\n",
      "Profile 11: M_k=3, #policies=7, theta_k=2.85936\n",
      ": RPS size: 0\n",
      "Profile 12: M_k=2, #policies=3, theta_k=2.26666\n",
      ": RPS size: 4\n",
      "Profile 13: M_k=3, #policies=7, theta_k=2.97691\n",
      ": RPS size: 0\n",
      "Profile 14: M_k=3, #policies=7, theta_k=3.89039\n",
      ": RPS size: 0\n",
      "Profile 15: M_k=4, #policies=15, theta_k=9.34637\n",
      ": RPS size: 0\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T05:38:48.281958Z",
     "start_time": "2025-06-21T05:38:48.267014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from itertools import product\n",
    "# Step 4: Cross-product of valid RashomonSets (partial Rashomon set)\n",
    "nonempty_idx = [i for i, rp in enumerate(R_profiles) if len(rp) > 0]\n",
    "nonempty_profiles = [R_profiles[i] for i in nonempty_idx]\n",
    "R_set_partial = list(product(*[range(len(rp)) for rp in nonempty_profiles]))\n",
    "print(f\"Found {len(R_set_partial)} Rashomon sets across {len(nonempty_profiles)} nonempty profiles.\")\n",
    "print(f\"Nonempty profile indices: {nonempty_idx}\")\n",
    "print(f\"Total profiles (partitions): {len(R_profiles)}\")\n",
    "print(f\"Profiles with nonempty RPS: {len(nonempty_profiles)}\")\n"
   ],
   "id": "a782f428c90f72f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 65536 Rashomon sets across 10 nonempty profiles.\n",
      "Nonempty profile indices: [0, 1, 2, 3, 4, 5, 7, 8, 9, 11]\n",
      "Total profiles (partitions): 15\n",
      "Profiles with nonempty RPS: 10\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can now set this up in a larger wrapper function too:",
   "id": "81dfe39194dc15fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T05:39:11.697678Z",
     "start_time": "2025-06-21T05:39:11.596446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from observed_RPS import observed_rps\n",
    "R_set_partial, R_profiles, nonempty_idx, profiles = observed_rps(\n",
    "    M, R_vec, H, D1, y1, lambda_r, eps=0.05\n",
    ")"
   ],
   "id": "7fe769f6a7cac52",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile 1: M_k=1, #policies=1, theta_k=1.85380\n",
      "Profile 2: M_k=1, #policies=1, theta_k=1.55939\n",
      "Profile 3: M_k=2, #policies=3, theta_k=2.28029\n",
      "Profile 4: M_k=1, #policies=1, theta_k=1.30117\n",
      "Profile 5: M_k=2, #policies=3, theta_k=2.28977\n",
      "Profile 6: M_k=2, #policies=3, theta_k=1.79353\n",
      "Profile 7: M_k=3, #policies=7, theta_k=3.94933\n",
      "Profile 8: M_k=1, #policies=1, theta_k=1.40522\n",
      "Profile 9: M_k=2, #policies=3, theta_k=2.09451\n",
      "Profile 10: M_k=2, #policies=3, theta_k=2.19088\n",
      "Profile 11: M_k=3, #policies=7, theta_k=2.85936\n",
      "Profile 12: M_k=2, #policies=3, theta_k=2.26666\n",
      "Profile 13: M_k=3, #policies=7, theta_k=2.97691\n",
      "Profile 14: M_k=3, #policies=7, theta_k=3.89039\n",
      "Profile 15: M_k=4, #policies=15, theta_k=9.34637\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We demonstrate a main wrapper call, as we would use from the original Rashomon module, but we note that the function operates unexpectedly because we don't have data for a number of the profiles. (We end up with an empty Rashomon Partition Set).",
   "id": "550fb5a98dfcbeb8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Doesn't work!! Because only have data on a small subspace, and quits out when we can't make a pooling decision\n",
    "# Call main RAggregate function\n",
    "R_set, R_profiles = RAggregate(\n",
    "    M=M,\n",
    "    R=R_vec,\n",
    "    H=H,\n",
    "    D=D1,\n",
    "    y=y1,\n",
    "    theta=theta_global,\n",
    "    reg=lambda_r,\n",
    "    verbose=True,\n",
    ")"
   ],
   "id": "c61165e5ea76d402",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Second-wave allocation",
   "id": "e216bc08a51b126c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3542cb980c3cfd85"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
