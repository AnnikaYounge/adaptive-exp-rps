{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Two-wave RPS Algorithm",
   "id": "3821486d2362d731"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T18:15:23.370196Z",
     "start_time": "2025-06-28T18:15:23.360616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from rashomon.hasse import enumerate_policies\n",
    "from allocation import compute_boundary_probs, allocate_wave, assign_treatments\n",
    "from data_gen import get_beta_underlying_causal, generate_outcomes"
   ],
   "id": "5242302164f6663e",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. First-wave allocation",
   "id": "df6535a12f16459b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T18:16:01.576186Z",
     "start_time": "2025-06-28T18:16:01.565459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# get lattice\n",
    "M = 4\n",
    "R = [2,3,3,4]\n",
    "\n",
    "R_vec = np.full(M, R) if np.isscalar(R) else np.array(R) # allow for heterogeneity in levels\n",
    "assert R_vec.shape == (M,)\n",
    "policies = enumerate_policies(M, R)\n",
    "\n",
    "K = len(policies)\n",
    "print(f\"Found K = {K} policies (each policy is an {M}-tuple).\")\n",
    "H = 5  # sparsity parameter used inside compute_boundary_probs TODO choice\n",
    "n1 = 500  # total first‐wave sample size"
   ],
   "id": "193da5cfc81258b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found K = 72 policies (each policy is an 4-tuple).\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Compute first‐wave allocation**: We need R_i for each feature (here R_i = R for i=0,…,M-1), then we call `compute_boundary_probs` -> `allocate_first_wave`. We get `n1_alloc`: an array of length K summing to n1.",
   "id": "c7c2ae3f8cef0d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T18:16:02.369966Z",
     "start_time": "2025-06-28T18:16:02.362586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "boundary_probs = compute_boundary_probs(policies, R, H)\n",
    "n1_alloc = allocate_wave(boundary_probs, n1)\n",
    "print(f\"First‐wave allocation sums to {int(n1_alloc.sum())} (should be {n1}).\")"
   ],
   "id": "6dcc0a9d2d342815",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First‐wave allocation sums to 500 (should be 500).\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Simulating first-wave outcomes",
   "id": "db2cf32df91f077f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We generate a np.array `beta` of true effects for each node. We pass our lattice `policies`, `M` and `R`, and then specify a `kind` of underlying causal model.\n",
    "\n",
    "There are a range of options, all of which are continuous and non-trivial: they exhibit locally correlated effects and avoid brittle cancellations in effects. The options range from simple (polynomial, gaussian, basic interaction) to complex (radial basis function, mimic of a simple neural-net-like function)"
   ],
   "id": "783d2bf35aa2605f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T18:16:03.301568Z",
     "start_time": "2025-06-28T18:16:03.298474Z"
    }
   },
   "cell_type": "code",
   "source": "beta = get_beta_underlying_causal(policies, M, R, kind=\"gauss\")",
   "id": "37f3b09738cfdc4e",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T18:16:03.506090Z",
     "start_time": "2025-06-28T18:16:03.503870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Not in use: different distribution for each true pool from a random 'true' partition sigma_true. Not used in this simulation due to our specifications on the underlying causal model (e.g. continuous, locally correlated effects, etc). Also needs changes on how it constructs a true partition.\n",
    "\n",
    "# partition_seed = 123\n",
    "# sigma_true, pi_pools_true, pi_policies_true = generate_true_partition(policies, R,random_seed=partition_seed)\n",
    "# beta = get_beta_piecewise(policies, sigma_true, pi_pools_true, pi_policies_true, 0.5, 1, 10)"
   ],
   "id": "c0124dca96a59a16",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Get outcomes**: we now track the first-wave assignment and generate the outcomes with additional noise",
   "id": "96414d2d89e71d3c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T18:16:03.935557Z",
     "start_time": "2025-06-28T18:16:03.931258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# now build first-wave assignment vector D\n",
    "policies = np.array(enumerate_policies(M, R))  # (K, M)\n",
    "D = assign_treatments(n1_alloc)  # (N1, M)\n",
    "print(\"D shape:\", D.shape)"
   ],
   "id": "4cb7d7c5a1785390",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D shape: (500,)\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T18:16:04.113598Z",
     "start_time": "2025-06-28T18:16:04.107923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# generate outcomes y1\n",
    "sigma_noise = 5\n",
    "outcome_seed = 53\n",
    "y = generate_outcomes(D=D, beta=beta, sigma_noise=sigma_noise, random_seed=outcome_seed)\n",
    "print(\"Overall mean outcome:\", np.mean(y))\n",
    "print(\"Overall std outcome:\", np.std(y))"
   ],
   "id": "a9a34a6d63a9e449",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall mean outcome: -0.07506026338360178\n",
      "Overall std outcome: 5.3521106652966255\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. RPS for profiles with data",
   "id": "d65ec763ab104f39"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We now search for the optimal theta as given by a normalized loss and chosen epsilon. Need to already specify H and the regularization parameter.",
   "id": "30fbf3ae778f228f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T18:16:04.692884Z",
     "start_time": "2025-06-28T18:16:04.689933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lambda_r = 0.01\n",
    "eps = 0.05 # chosen tolerance"
   ],
   "id": "bf3efc3328c7bf10",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T18:16:04.896973Z",
     "start_time": "2025-06-28T18:16:04.894360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from rashomon.hasse import enumerate_policies, enumerate_profiles, policy_to_profile\n",
    "from rashomon.aggregate import (\n",
    "    RAggregate_profile,\n",
    "    subset_data,\n",
    "    find_profile_lower_bound,\n",
    ")\n",
    "from rashomon import loss"
   ],
   "id": "6dbeb16e2bb5cc0f",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### RAggregate_observed walk through",
   "id": "6f2552dfd48993f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T18:16:05.439343Z",
     "start_time": "2025-06-28T18:16:05.436019Z"
    }
   },
   "cell_type": "code",
   "source": "verbose = True",
   "id": "f6a5945de10ae4e7",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T18:16:05.681949Z",
     "start_time": "2025-06-28T18:16:05.677371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "N = len(D)\n",
    "M = len(policies[0])\n",
    "\n",
    "# Build profiles and maps between policies\n",
    "profiles, profile_map = enumerate_profiles(M)\n",
    "profile_to_policies = {k: [] for k in range(len(profiles))}\n",
    "profile_to_indices = {k: [] for k in range(len(profiles))}\n",
    "for i, pol in enumerate(policies):\n",
    "    pid = profile_map[policy_to_profile(pol)]\n",
    "    profile_to_policies[pid].append(pol)\n",
    "    profile_to_indices[pid].append(i)"
   ],
   "id": "1749e681207bae85",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T18:16:05.994256Z",
     "start_time": "2025-06-28T18:16:05.976876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get just the profiles and profile_ids with data and track losses\n",
    "valid_pids = []\n",
    "lower_bounds = []\n",
    "for profile_id, profile in enumerate(profiles):\n",
    "    Dk, yk = subset_data(D, y, profile_to_indices[profile_id]) # using rashomon.aggregate, get correct subset of data\n",
    "    if Dk is None:\n",
    "        continue\n",
    "    mask = np.array(profile, dtype=bool)\n",
    "    # corresponding policies for this profile id\n",
    "    reduced_policies = [tuple(np.array(p)[mask]) for p in profile_to_policies[profile_id]]\n",
    "\n",
    "    # get losses and track lower bounds\n",
    "    pm = loss.compute_policy_means(Dk, yk, len(reduced_policies))\n",
    "    profile_lb = find_profile_lower_bound(Dk, yk, pm)\n",
    "    lower_bounds.append(profile_lb / N)\n",
    "    valid_pids.append(profile_id)\n",
    "\n",
    "lower_bounds = np.array(lower_bounds)\n",
    "best_loss = lower_bounds.min()\n",
    "print(f\"best_loss = {best_loss:.5f}\")\n",
    "lower_bounds = np.array(lower_bounds)\n",
    "total_lb = lower_bounds.sum()"
   ],
   "id": "6eec71b727a66623",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_loss = 0.25737\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T18:16:06.291392Z",
     "start_time": "2025-06-28T18:16:06.289182Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# calculate rashomon threshold\n",
    "theta_global = total_lb * (1 + eps) # get loss threshold in absolute reference to sum of lower bounds\n",
    "if verbose:\n",
    "    print(f\"theta_global = {theta_global:.5f} from sum of lower bounds {total_lb:.5f}\")"
   ],
   "id": "beff4c16f6f47792",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_global = 26.17569 from sum of lower bounds 24.92923\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Construct the full RPS",
   "id": "9513341e613d6d4d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T18:16:07.495601Z",
     "start_time": "2025-06-28T18:16:07.460654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "R_profiles = []\n",
    "nonempty_profile_ids = []\n",
    "for i, profile_id in enumerate(valid_pids):\n",
    "    profile_mask = np.array(profiles[profile_id], dtype=bool)\n",
    "    M_k = profile_mask.sum()\n",
    "\n",
    "    # Compute reduced policies using only active features for this profile\n",
    "    reduced_policies = [tuple(np.array(p)[profile_mask]) for p in profile_to_policies[profile_id]]\n",
    "\n",
    "    # Compute number of levels for each local (profile) feature\n",
    "    R_k = np.array([len(set([p[feat] for p in reduced_policies])) for feat in range(M_k)])\n",
    "\n",
    "    # Remap each feature in reduced_policies to contiguous 0-based values\n",
    "    reduced_policies_arr = np.array(reduced_policies)\n",
    "    for j in range(reduced_policies_arr.shape[1]):\n",
    "        _, reduced_policies_arr[:, j] = np.unique(reduced_policies_arr[:, j], return_inverse=True)\n",
    "    reduced_policies = [tuple(row) for row in reduced_policies_arr]\n",
    "    R_k = np.array([len(np.unique(reduced_policies_arr[:, j])) for j in range(M_k)])\n",
    "\n",
    "    # Value-mapping-based subsetting and remapping\n",
    "    # This gives Dk (local indices) and yk (outcomes)\n",
    "    policy_indices_this_profile = profile_to_indices[profile_id]\n",
    "    mask = np.isin(D, policy_indices_this_profile)\n",
    "    Dk = D[mask]\n",
    "    yk = y[mask]\n",
    "\n",
    "    # Now remap Dk from global policy indices to local indices in reduced_policies\n",
    "    Dk = np.asarray(Dk).reshape(-1)\n",
    "    policy_map = {idx: j for j, idx in enumerate(policy_indices_this_profile)}\n",
    "    assert all(ix in policy_map for ix in Dk), f\"Found Dk values not in mapping for profile {profile_id}\"\n",
    "    Dk_local = np.vectorize(policy_map.get)(Dk)      # map to local indices, shape (n,)\n",
    "    assert yk.shape[0] == Dk_local.shape[0], \"Dk_local and yk must have the same length\"\n",
    "\n",
    "\n",
    "    # Need to have Dk as a 1D array for the loss functions\n",
    "    # Compute policy means with local indices\n",
    "    pm = loss.compute_policy_means(Dk_local, yk, len(reduced_policies))\n",
    "    assert pm.shape[0] == len(reduced_policies), \"policy_means length mismatch\"\n",
    "\n",
    "    # get profile threshold\n",
    "    theta_k = max(0.0, theta_global - (total_lb - lower_bounds[i]))\n",
    "\n",
    "    # Need to reshape np array because the RAggregate_profile expects shape (n,1)\n",
    "    Dk_local = Dk_local.reshape(-1, 1)\n",
    "    yk = yk.reshape(-1, 1)\n",
    "    # get rashomon set for each profile\n",
    "    rashomon_profile = RAggregate_profile(\n",
    "        M=M_k,\n",
    "        R=R_k,\n",
    "        H=H,\n",
    "        D=Dk_local,  # Already local indices\n",
    "        y=yk,\n",
    "        theta=theta_k,\n",
    "        profile=tuple(profiles[profile_id]),\n",
    "        reg=lambda_r,\n",
    "        policies=reduced_policies,\n",
    "        policy_means=pm,\n",
    "        normalize=N\n",
    "    )\n",
    "\n",
    "    # calculate losses for non-empty profiles and add to list of profiles\n",
    "    Dk = np.asarray(Dk).reshape(-1) # loss functions again want a 1d array for D, but keep y 2d\n",
    "    if len(rashomon_profile) > 0:\n",
    "        rashomon_profile.calculate_loss(Dk_local, yk, reduced_policies, pm, lambda_r, normalize=N)\n",
    "        R_profiles.append(rashomon_profile)\n",
    "        nonempty_profile_ids.append(profile_id)\n",
    "    if verbose:\n",
    "        print(f\"Profile {profile_id}: M_k={M_k}, #policies={len(reduced_policies)}, theta_k={theta_k:.5f}, RPS size={len(rashomon_profile)}\")"
   ],
   "id": "18ba919ee77399d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile 1: M_k=1, #policies=3, theta_k=2.63517, RPS size=2\n",
      "Profile 2: M_k=1, #policies=2, theta_k=1.69617, RPS size=1\n",
      "Profile 3: M_k=2, #policies=6, theta_k=2.62537, RPS size=2\n",
      "Profile 4: M_k=1, #policies=2, theta_k=1.50383, RPS size=1\n",
      "Profile 5: M_k=2, #policies=6, theta_k=3.00661, RPS size=2\n",
      "Profile 6: M_k=2, #policies=4, theta_k=2.72556, RPS size=1\n",
      "Profile 7: M_k=3, #policies=12, theta_k=5.52833, RPS size=2\n",
      "Profile 9: M_k=2, #policies=3, theta_k=2.12428, RPS size=2\n",
      "Profile 10: M_k=2, #policies=2, theta_k=1.65157, RPS size=1\n",
      "Profile 11: M_k=3, #policies=6, theta_k=3.34851, RPS size=2\n",
      "Profile 12: M_k=2, #policies=2, theta_k=1.51779, RPS size=1\n",
      "Profile 13: M_k=3, #policies=6, theta_k=3.59946, RPS size=2\n",
      "Profile 14: M_k=3, #policies=4, theta_k=3.03184, RPS size=1\n",
      "Profile 15: M_k=4, #policies=12, theta_k=7.38522, RPS size=2\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T18:16:08.483544Z",
     "start_time": "2025-06-28T18:16:08.479738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# if none of the profiles have a rashomon set, observed RPS is empty\n",
    "if len(R_profiles) == 0:\n",
    "    if verbose:\n",
    "        print(\"No profiles have feasible Rashomon sets; global RPS is empty.\")"
   ],
   "id": "aebb5f736b7a1180",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T18:16:08.883532Z",
     "start_time": "2025-06-28T18:16:08.880104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "excluded_profiles = [profile_id for profile_id in valid_pids if profile_id not in nonempty_profile_ids]\n",
    "if verbose:\n",
    "    if len(excluded_profiles) > 0:\n",
    "        print(f\"Skipped profile number due to empty Rashomon set: {excluded_profiles}\")\n",
    "    else:\n",
    "        print(\"All profiles with data have non-empty Rashomon sets.\")"
   ],
   "id": "f3545a0db14eaccc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All profiles with data have non-empty Rashomon sets.\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T18:16:09.465870Z",
     "start_time": "2025-06-28T18:16:09.462132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for idx, rp in enumerate(R_profiles):\n",
    "    losses = np.array(rp.loss)\n",
    "    print(f\"Profile {nonempty_profile_ids[idx]}: min loss = {losses.min():.4f}, max loss = {losses.max():.4f}\")"
   ],
   "id": "fd0a5c87c5b0480b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile 1: min loss = 1.4170, max loss = 1.4187\n",
      "Profile 2: min loss = 0.4697, max loss = 0.4697\n",
      "Profile 3: min loss = 1.4389, max loss = 1.4742\n",
      "Profile 4: min loss = 0.2774, max loss = 0.2774\n",
      "Profile 5: min loss = 1.8201, max loss = 2.0287\n",
      "Profile 6: min loss = 1.5191, max loss = 1.5191\n",
      "Profile 7: min loss = 4.4019, max loss = 5.2520\n",
      "Profile 9: min loss = 0.8927, max loss = 0.9078\n",
      "Profile 10: min loss = 0.4251, max loss = 0.4251\n",
      "Profile 11: min loss = 2.1309, max loss = 2.1621\n",
      "Profile 12: min loss = 0.2913, max loss = 0.2913\n",
      "Profile 13: min loss = 2.4130, max loss = 3.0275\n",
      "Profile 14: min loss = 1.8254, max loss = 1.8254\n",
      "Profile 15: min loss = 6.2588, max loss = 6.4168\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T18:16:10.174844Z",
     "start_time": "2025-06-28T18:16:10.168618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assemble observed RPS via find_feasible_combinations from RAggregate\n",
    "from rashomon.aggregate import find_feasible_combinations\n",
    "R_set = find_feasible_combinations(R_profiles, theta_global, H)\n",
    "if verbose:\n",
    "    print(f\"RPS has: {len(R_set)} feasible partitions over {len(R_profiles)} observed profiles.\")"
   ],
   "id": "863353d88dea3fe5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RPS has: 0 feasible partitions over 14 observed profiles.\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We also demonstrate a main wrapper call, as we would use from the original Rashomon module, but we note that the function operates unexpectedly because we don't have data for a number of the profiles. (We end up with an empty Rashomon Partition Set).",
   "id": "550fb5a98dfcbeb8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T18:16:13.681082Z",
     "start_time": "2025-06-28T18:16:13.675054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Old algorithm doens't work. Requires data on everything, and feasible_combinations and loss functions throw an error when dealing with a smaller subsection.\n",
    "# R_set_empty, R_profiles_empty = RAggregate(\n",
    "#     M=M,\n",
    "#     R=R_vec,\n",
    "#     H=H,\n",
    "#     D=D1,\n",
    "#     y=y1,\n",
    "#     theta=theta_global,\n",
    "#     reg=lambda_r,\n",
    "#     verbose=True,\n",
    "# )"
   ],
   "id": "c61165e5ea76d402",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Second-wave allocation",
   "id": "e216bc08a51b126c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# See data_gen and allocation files again",
   "id": "3542cb980c3cfd85",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
