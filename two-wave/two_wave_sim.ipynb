{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ALGORITHM 1: TWO-WAVE",
   "id": "a3fb333436a21a99"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T17:44:30.106279Z",
     "start_time": "2025-07-14T17:44:29.316446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from rashomon.hasse import policy_to_profile, enumerate_policies, enumerate_profiles\n",
    "from datagen import phi, generate_data_from_assignments\n",
    "from boundary import (\n",
    "    compute_boundary_probabilities, get_allocations,\n",
    "    create_assignments_from_alloc, get_policy_neighbors, compute_global_boundary_matrix\n",
    ")\n",
    "from helpers_rps import (\n",
    "    subset_wave_data_by_profile, compute_profile_policy_outcomes,\n",
    "    build_global_wave_data, get_observed_profiles\n",
    ")\n",
    "from enumerate_rps import construct_RPS_adaptive\n",
    "from evaluation import get_partition_losses"
   ],
   "id": "e407fdc761783953",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 1: Enumerate policies and profiles and build index mappings",
   "id": "4e3067ba3b62a2cd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T17:44:41.713468Z",
     "start_time": "2025-07-14T17:44:41.707266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "M = 3 # number of features\n",
    "R = np.array([4, 3, 3]) # levels per feature\n",
    "lambda_reg = 0.1 # reg parameter\n",
    "epsilon = 0.05 # tolerance off MAP\n",
    "n1 = 100 # units for first wave\n",
    "n2 = 100  # units for second wave\n",
    "theta_start = 0.7\n",
    "verbose = False\n",
    "\n",
    "# enumerate all policies and profiles\n",
    "all_policies = enumerate_policies(M, R)\n",
    "num_policies = len(all_policies)\n",
    "profiles, profile_map = enumerate_profiles(M)\n",
    "num_profiles = len(profiles)\n",
    "\n",
    "# profile index mappings\n",
    "policies_profiles = {}\n",
    "policies_ids_profiles = {}\n",
    "for k, profile in enumerate(profiles):\n",
    "    policy_indices = [i for i, p in enumerate(all_policies) if policy_to_profile(p) == profile]\n",
    "    policies_ids_profiles[k] = policy_indices\n",
    "    policies_profiles[k] = [all_policies[i] for i in policy_indices]\n",
    "\n",
    "# max pool size checks for sparsity\n",
    "max_pool_size = max(len(policies) for policies in policies_profiles.values())\n",
    "largest_profile = {k: len(policies) for k, policies in policies_profiles.items()}\n",
    "if verbose:\n",
    "    print(f\"Profile-wise max pools: {largest_profile}\")\n",
    "    print(f\"Max possible pool size for a profile: {max_pool_size}\")\n",
    "\n",
    "H = max_pool_size # can set H to be max possible pool size to be conservative\n",
    "\n",
    "# masks per profile for correct splitting procedure\n",
    "policies_profiles_masked = {}\n",
    "for k, profile in enumerate(profiles):\n",
    "    profile_mask = [bool(v) for v in profile] # t/f map of which features are active\n",
    "    masked_policies = [tuple([pol[i] for i in range(M) if profile_mask[i]]) for pol in policies_profiles[k]] # list of policies but they now are only the active features\n",
    "    policies_profiles_masked[k] = masked_policies # holds, at a given profile index, the masked policies for that profile"
   ],
   "id": "2efb2423377db490",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 2: Get true outcomes and top arms",
   "id": "4222024f3c5c23dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T17:44:42.175208Z",
     "start_time": "2025-07-14T17:44:42.171967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# vector of true policy outcomes\n",
    "oracle_outcomes = np.array([phi(p) for p in all_policies])\n",
    "\n",
    "# oracle rank mapped to policy index (so index 1 gives element best policy index)\n",
    "oracle_rank_to_policy = np.argsort(-oracle_outcomes)\n",
    "\n",
    "# policy index mapped to oracle rank (so index 1 gives oracle rank for policy 1)\n",
    "oracle_policy_to_rank = np.empty_like(oracle_rank_to_policy)\n",
    "oracle_policy_to_rank[oracle_rank_to_policy] = np.arange(1, len(oracle_outcomes)+1)\n",
    "\n",
    "# Top-k indices, policies, and values\n",
    "top_k = 10\n",
    "top_k_indices = oracle_rank_to_policy[:top_k]\n",
    "top_k_policies = [all_policies[i] for i in top_k_indices]\n",
    "top_k_values = oracle_outcomes[top_k_indices]\n",
    "\n",
    "# Overview of the rank, index, and profiles of the top policies\n",
    "if verbose:\n",
    "    print(\"Top-k best policies and their profiles:\")\n",
    "    for rank, idx in enumerate(top_k_indices, 1):\n",
    "        policy = all_policies[idx]\n",
    "        profile = policy_to_profile(policy)\n",
    "        print(f\"Rank {rank}: Policy idx {idx}, Policy {[int(i) for i in policy]}, Profile {profile}\")"
   ],
   "id": "937e9834d379f2c9",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 3: Get boundary probabilities and generate from assignments",
   "id": "2dcda77ada9c3865"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T17:44:42.507858Z",
     "start_time": "2025-07-14T17:44:42.503480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# compute theoretical boundary probabilities using equation\n",
    "boundary_probs = compute_boundary_probabilities(all_policies, R, H)\n",
    "\n",
    "# integer allocation across all policies, sums to n1\n",
    "alloc1 = get_allocations(boundary_probs, n1)\n",
    "if verbose: print(f\"Total allocated: {alloc1.sum()} (should be {n1})\")\n",
    "\n",
    "# generate assignments for wave 1\n",
    "D1 = create_assignments_from_alloc(alloc1)  # shape (n1, 1)\n",
    "X1, y1 = generate_data_from_assignments(D1, all_policies, oracle_outcomes, sig=1.0)\n",
    "\n",
    "if verbose:\n",
    "    print(f\"Wave 1 assignments (policy indices): {D1[:10].flatten()}\")\n",
    "    print(f\"Total n_1: {len(D1)} (should match allocation sum: {alloc1.sum()})\")"
   ],
   "id": "f149017b28062ab1",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T17:44:42.690266Z",
     "start_time": "2025-07-14T17:44:42.687492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# get the profiles that are observed and their corresponding observed policies\n",
    "observed_policies_per_profile, observed_profiles = get_observed_profiles(D1, all_policies)\n",
    "\n",
    "# get max pool size with only observed policies\n",
    "max_observed_pool_size = max(len(policies) for policies in observed_policies_per_profile.values())\n",
    "\n",
    "# output information about coverage of RPS\n",
    "if verbose:\n",
    "    print(f\"Number of observed profiles out of total: {len(observed_profiles)} out of {num_profiles}\")\n",
    "    print(f\"Number of observed policies out of total: {np.sum([len(policies) for policies in observed_policies_per_profile])} out of {num_policies}\")\n",
    "    print(f\"The maximum possible pool size using just observed policies is now {max_observed_pool_size}.\")\n",
    "\n",
    "    print(\"\\nAre all top-k best profiles observed?\")\n",
    "    for rank, idx in enumerate(top_k_indices, 1):\n",
    "        policy = all_policies[idx]\n",
    "        prof = policy_to_profile(policy)\n",
    "        print(f\"Best Policy {idx}: Profile {prof}, Observed? {prof in observed_profiles}\")"
   ],
   "id": "ddc3b67eb4a0c9ad",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 4: Enumerate RPS with first-wave data",
   "id": "81d89714d3f2a2bb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T17:44:43.117297Z",
     "start_time": "2025-07-14T17:44:43.113237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "D1_profiles, y1_profiles, global_to_local1 = subset_wave_data_by_profile(D1, y1, policies_ids_profiles)\n",
    "\n",
    "profile_policy_outcomes1 = compute_profile_policy_outcomes(D1_profiles, y1_profiles, policies_profiles)\n",
    "\n",
    "# get max observed outcomes within each profile\n",
    "if verbose:\n",
    "    for k in profile_policy_outcomes1:\n",
    "        pm = profile_policy_outcomes1[k]\n",
    "        means = pm[:,0] / np.maximum(pm[:,1], 1)\n",
    "        print(f\"Profile {k}: Max observed mean = {means.max():.3f}\")\n",
    "\n",
    "D1_full, y1_full = build_global_wave_data(D1_profiles, y1_profiles, policies_ids_profiles)"
   ],
   "id": "79cbd394b401836c",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T17:44:44.076673Z",
     "start_time": "2025-07-14T17:44:43.272707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "theta_global = 5\n",
    "\n",
    "R_set, R_profiles, theta_final, found_best, theta_trace, rps_size_trace = construct_RPS_adaptive(\n",
    "    M, R, H, D1_full, y1_full, top_k, policies_profiles_masked, policies_ids_profiles,\n",
    "    profiles, all_policies, top_k_indices, theta_global, reg=lambda_reg, adaptive=False, verbose=verbose, recovery_type=\"arm\"\n",
    ")\n",
    "if verbose:\n",
    "    print(f\"First-wave Rashomon set: {len(R_set)} feasible global partitions (combinations of per-profile poolings).\")\n",
    "    for k, rprof in enumerate(R_profiles):\n",
    "        print(f\"Profile {k}: {len(rprof)} poolings in RPS (if observed)\")"
   ],
   "id": "9c5fc81632b0ce48",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T17:44:44.110063Z",
     "start_time": "2025-07-14T17:44:44.104224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# After enumerating RPS (wave 1)\n",
    "partition_losses, posterior_weights = get_partition_losses(R_set, R_profiles)\n",
    "\n",
    "# Identify MAP partition (lowest-loss)\n",
    "map_idx = np.argmin(partition_losses)\n",
    "map_loss = partition_losses[map_idx]\n",
    "\n",
    "if verbose:\n",
    "    print(f\"MAP partition loss: {map_loss:.6f}\")\n",
    "    print(f\"Theta used for enumeration: {theta_global:.6f}\")"
   ],
   "id": "858ef6f40d520ef0",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 5: Calculate new boundary probabilities and generate next wave",
   "id": "e400fdbe2fafaabc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T17:44:46.193198Z",
     "start_time": "2025-07-14T17:44:45.572340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# look at neighbors and calculate probability on a boundary for each policy\n",
    "neighbors = get_policy_neighbors(all_policies)\n",
    "\n",
    "# TODO switch around so also deals with being neighbors with someone outside of your profile\n",
    "boundary_matrix_1 = compute_global_boundary_matrix(\n",
    "    R_set, R_profiles, neighbors, profiles, policies_profiles_masked, policies_ids_profiles, all_policies\n",
    ")\n",
    "\n",
    "# get T/F mask of the matrix of counts of number of boundaries\n",
    "binary_boundary_matrix_1 = (boundary_matrix_1 > 0).astype(float)\n",
    "partition_losses, posterior_weights_1 = get_partition_losses(R_set, R_profiles)\n",
    "posterior_boundary_probs_1 = np.average(binary_boundary_matrix_1, axis=0, weights=posterior_weights_1)\n",
    "\n",
    "# to avoid small numerical errors, round to 8 decimal\n",
    "posterior_boundary_probs_1 = np.round(posterior_boundary_probs_1, decimals=8)"
   ],
   "id": "b18d06a3166f793b",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T17:44:46.228102Z",
     "start_time": "2025-07-14T17:44:46.225213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if verbose:\n",
    "    assert posterior_boundary_probs_1.shape == (len(all_policies),), \"Posterior vector shape mismatch\"\n",
    "    # 2. Check range and sum\n",
    "    print(\"Posterior boundary min/max:\", posterior_boundary_probs_1.min(), posterior_boundary_probs_1.max())\n",
    "    # 3. Check for degenerate values\n",
    "    assert np.all((posterior_boundary_probs_1 >= 0) & (posterior_boundary_probs_1 <= 1)), \"Probabilities out of bounds\""
   ],
   "id": "24276ce747a93929",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T17:44:46.409223Z",
     "start_time": "2025-07-14T17:44:46.402803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# get allocations and create assignments, then generate data, for the next wave\n",
    "alloc2 = get_allocations(posterior_boundary_probs_1, n2)\n",
    "D2 = create_assignments_from_alloc(alloc2)\n",
    "X2, y2 = generate_data_from_assignments(D2, all_policies, oracle_outcomes, sig=1.0)\n",
    "\n",
    "if verbose:\n",
    "    print(f\"Second-wave assignments (policy indices): {D2[:10].flatten()}\")\n",
    "    print(f\"Total n_2: {len(D2)} (should match allocation sum: {alloc2.sum()})\")"
   ],
   "id": "47c7d5ee37c7353e",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Step 6: Construct updated RPS",
   "id": "3970842300219286"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T17:44:47.014918Z",
     "start_time": "2025-07-14T17:44:47.010001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "D_total = np.vstack([D1, D2])\n",
    "y_total = np.vstack([y1, y2])\n",
    "\n",
    "# Subset all observed data by profile and remap to global policy indices\n",
    "D_total_profiles, y_total_profiles, global_to_local_total = subset_wave_data_by_profile(\n",
    "    D_total, y_total, policies_ids_profiles\n",
    ")\n",
    "profile_policy_outcomes_total = compute_profile_policy_outcomes(D_total_profiles, y_total_profiles, policies_profiles)\n",
    "# Map all profile-local indices back to global for RPS construction\n",
    "D_total_full, y_total_full = build_global_wave_data(D_total_profiles, y_total_profiles, policies_ids_profiles)"
   ],
   "id": "f75691f970afe49a",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T17:44:48.441726Z",
     "start_time": "2025-07-14T17:44:47.401131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO depending on RPS change, may need to redefine theta!\n",
    "R_set_2, R_profiles_2, theta_final_2, found_best_2, theta_trace_2, rps_size_trace_2 = construct_RPS_adaptive(\n",
    "    M, R, H, D_total_full, y_total_full, top_k, policies_profiles_masked, policies_ids_profiles,\n",
    "    profiles, all_policies, top_k_indices, theta_global, reg=lambda_reg, adaptive=False, verbose=verbose, recovery_type=\"arm\"\n",
    ")\n",
    "if verbose:\n",
    "    print(f\"Second-wave Rashomon set: {len(R_set_2)} feasible global partitions (with all data).\")\n",
    "    for k, rprof in enumerate(R_profiles_2):\n",
    "        print(f\"Profile {k}: {len(rprof)} poolings in RPS (if observed)\")\n",
    "\n",
    "# After enumerating RPS (wave 1)\n",
    "partition_losses2, posterior_weights = get_partition_losses(R_set_2, R_profiles_2)\n",
    "\n",
    "# Identify MAP partition (lowest-loss)\n",
    "map_idx = np.argmin(partition_losses2)\n",
    "map_loss = partition_losses2[map_idx]\n",
    "\n",
    "if verbose:\n",
    "    print(f\"MAP partition loss: {map_loss:.6f}\")\n",
    "    print(f\"Theta used for enumeration: {theta_final_2:.6f}\")"
   ],
   "id": "484cf9d94590b68",
   "outputs": [],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
