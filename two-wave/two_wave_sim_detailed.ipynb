{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Two-wave RPS Algorithm Walk-Through",
   "id": "3821486d2362d731"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T20:50:15.425864Z",
     "start_time": "2025-06-28T20:50:15.386146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from rashomon.hasse import enumerate_policies\n",
    "import allocation\n",
    "from data_gen import get_beta_underlying_causal, generate_outcomes"
   ],
   "id": "5242302164f6663e",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. First-wave allocation",
   "id": "df6535a12f16459b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T20:50:16.046712Z",
     "start_time": "2025-06-28T20:50:16.044217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "M = 4\n",
    "R = [2,3,3,4]\n",
    "H = 20  # sparsity parameter\n",
    "n1 = 500  # total first‐wave sample size"
   ],
   "id": "4a09643623ab4e71",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T20:50:16.201645Z",
     "start_time": "2025-06-28T20:50:16.199308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "R_vec = np.full(M, R) if np.isscalar(R) else np.array(R) # allow for heterogeneity in levels\n",
    "assert R_vec.shape == (M,)\n",
    "policies = enumerate_policies(M, R)"
   ],
   "id": "193da5cfc81258b5",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T20:50:16.372602Z",
     "start_time": "2025-06-28T20:50:16.370248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "K = len(policies)\n",
    "print(f\"Found K = {K} policies (each policy is an {M}-tuple).\")"
   ],
   "id": "60b17ffb539766c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found K = 72 policies (each policy is an 4-tuple).\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.1. Compute initial boundary probabilities",
   "id": "efa1b9357e39e38f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T20:50:16.970995Z",
     "start_time": "2025-06-28T20:50:16.966828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MODULAR version: boundary_probs_1 = allocation.compute_initial_boundary_probs(policies, R, H)\n",
    "\n",
    "K = len(policies)\n",
    "if np.isscalar(R):\n",
    "    R_arr = np.array([R] * M, dtype=int)\n",
    "else:\n",
    "    R_arr = np.array(R, dtype=int)\n",
    "    if R_arr.size != M:\n",
    "        raise ValueError(f\"R must be an int or list/array of length M={M}, instead got size {R_arr.size}.\")\n",
    "\n",
    "boundary_probs_1 = np.zeros(K, dtype=float)\n",
    "for idx in range(K):\n",
    "    v = policies[idx]\n",
    "\n",
    "    M = len(v)\n",
    "    # from formula in paper\n",
    "    term = 1.0\n",
    "    for i in range(M):\n",
    "        R_i = R_arr[i]\n",
    "        ratio = 2 * min(int(v[i]), R_i - 1 - int(v[i])) / (R_i - 1)\n",
    "        term *= (1 - ratio) ** (H - 1)\n",
    "    boundary_probs_1[idx] = 1 - term"
   ],
   "id": "6dcc0a9d2d342815",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.2 Allocate observations to policies",
   "id": "7ccef72dcd7d8a05"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T20:50:17.631086Z",
     "start_time": "2025-06-28T20:50:17.625382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MODULAR version: n1_alloc = allocation.allocate_wave(boundary_probs_1, n1)\n",
    "\n",
    "total_prob = boundary_probs_1.sum()\n",
    "if total_prob == 0:\n",
    "    raise ValueError(\"Sum of boundary probabilities is zero. Check H or R.\")\n",
    "normalized = boundary_probs_1 / total_prob\n",
    "\n",
    "# get direct allocation (could be fractional)\n",
    "dir_alloc = normalized * n1\n",
    "\n",
    "# floor to integers\n",
    "n1_alloc = np.floor(dir_alloc).astype(int)\n",
    "remainder = dir_alloc - n1_alloc\n",
    "shortage = n1 - int(n1_alloc.sum())\n",
    "\n",
    "# distribute the remaining samples to the largest remainders\n",
    "if shortage > 0:\n",
    "    idx_sorted = np.argsort(remainder)\n",
    "    top_indices = idx_sorted[-shortage:]\n",
    "    n1_alloc[top_indices] += 1\n",
    "\n",
    "print(f\"First‐wave allocation sums to {int(n1_alloc.sum())} (should be {n1}).\")"
   ],
   "id": "b785eae3a2e97097",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First‐wave allocation sums to 500 (should be 500).\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Simulating first-wave outcomes",
   "id": "db2cf32df91f077f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.1. Get underlying true outcomes for each policy",
   "id": "40b27de7369e0a89"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We generate a np.array `beta` of true effects for each node. We pass our lattice `policies`, `M` and `R`, and then specify a `kind` of underlying causal model.\n",
    "\n",
    "There are a range of options, all of which are continuous and non-trivial: they exhibit locally correlated effects and avoid brittle cancellations in effects. The options range from simple (polynomial, gaussian, basic interaction) to complex (radial basis function, mimic of a simple neural-net-like function)"
   ],
   "id": "783d2bf35aa2605f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T20:50:18.464051Z",
     "start_time": "2025-06-28T20:50:18.460787Z"
    }
   },
   "cell_type": "code",
   "source": "beta = get_beta_underlying_causal(policies, M, R, kind=\"gauss\")",
   "id": "37f3b09738cfdc4e",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We do not, as we did previously in the original Rashomon module, use a different distribution for each true pool from a random 'true' partition sigma_true. This is not used in this simulation due to our specifications on the underlying causal model (e.g. continuous, locally correlated effects, etc), but we nevertheless include first-pass code at generating a true partition and getting a piecewise beta if we did wish to include it.\n",
   "id": "b868d319dd663f4b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T20:50:18.770013Z",
     "start_time": "2025-06-28T20:50:18.768281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# partition_seed = 123\n",
    "# sigma_true, pi_pools_true, pi_policies_true = generate_true_partition(policies, R,random_seed=partition_seed)\n",
    "# beta = get_beta_piecewise(policies, sigma_true, pi_pools_true, pi_policies_true, 0.5, 1, 10)"
   ],
   "id": "c0124dca96a59a16",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.2. Get simulated outcomes",
   "id": "2da35411df714784"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We now track the first-wave assignment and generate the outcomes with additional noise",
   "id": "96414d2d89e71d3c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T20:50:19.222539Z",
     "start_time": "2025-06-28T20:50:19.219538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MODULAR version: D1 = allocation.assign_treatments(n1_alloc)\n",
    "\n",
    "total = int(n1_alloc.sum())\n",
    "D1 = np.zeros(total, dtype=int)\n",
    "pos = 0\n",
    "for idx, count in enumerate(n1_alloc):\n",
    "    if count > 0:\n",
    "        D1[pos : pos + count] = idx\n",
    "        pos += count\n",
    "\n",
    "print(\"D shape:\", D1.shape)"
   ],
   "id": "4cb7d7c5a1785390",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D shape: (500,)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T20:50:19.381624Z",
     "start_time": "2025-06-28T20:50:19.356785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# generate outcomes y1\n",
    "sigma_noise = 5\n",
    "outcome_seed = 53\n",
    "y1 = generate_outcomes(D=D1, beta=beta, sigma_noise=sigma_noise, random_seed=outcome_seed)\n",
    "print(\"Overall mean outcome:\", np.mean(y1))\n",
    "print(\"Overall std outcome:\", np.std(y1))"
   ],
   "id": "a9a34a6d63a9e449",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall mean outcome: -0.07506026338360178\n",
      "Overall std outcome: 5.3521106652966255\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Construct the RPS",
   "id": "d65ec763ab104f39"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T20:50:20.505464Z",
     "start_time": "2025-06-28T20:50:19.622121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from rashomon.hasse import enumerate_policies, enumerate_profiles, policy_to_profile\n",
    "from rashomon.aggregate import (\n",
    "    RAggregate_profile,\n",
    "    subset_data,\n",
    "    find_profile_lower_bound,\n",
    ")\n",
    "from rashomon import loss"
   ],
   "id": "6dbeb16e2bb5cc0f",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T20:50:20.517338Z",
     "start_time": "2025-06-28T20:50:20.515828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lambda_r = 0.01\n",
    "eps = 0.05 # chosen tolerance\n",
    "verbose = True"
   ],
   "id": "bf3efc3328c7bf10",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.1. Mapping profiles\n",
    "\n",
    "We build a map between profiles and policies to observe which profiles contain data and which are as yet unobserved."
   ],
   "id": "79d14a9611edccf9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T20:50:20.527425Z",
     "start_time": "2025-06-28T20:50:20.525295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "N = len(D1)\n",
    "M = len(policies[0])\n",
    "\n",
    "# Build profiles and maps between policies\n",
    "profiles, profile_map = enumerate_profiles(M)\n",
    "profile_to_policies = {k: [] for k in range(len(profiles))}\n",
    "profile_to_indices = {k: [] for k in range(len(profiles))}\n",
    "for i, pol in enumerate(policies):\n",
    "    pid = profile_map[policy_to_profile(pol)]\n",
    "    profile_to_policies[pid].append(pol)\n",
    "    profile_to_indices[pid].append(i)"
   ],
   "id": "1749e681207bae85",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.2. Find losses and get threshold theta",
   "id": "e4072950f741e8c3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We then find the lower bound losses for each profile. We subset the relevant data for the profile, mask and get the corresponding policies, and then compute the policy means to find the profile loss lower bound. We find the best loss overall and calculate the total sum of lower bounds.",
   "id": "e2d1abf7b35c14bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T20:50:20.686490Z",
     "start_time": "2025-06-28T20:50:20.678768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get just the profiles and profile_ids with data and track losses\n",
    "valid_pids = []\n",
    "lower_bounds = []\n",
    "for profile_id, profile in enumerate(profiles):\n",
    "    Dk, yk = subset_data(D1, y1, profile_to_indices[profile_id]) # using rashomon.aggregate, get correct subset of data\n",
    "    if Dk is None:\n",
    "        continue\n",
    "    mask = np.array(profile, dtype=bool)\n",
    "    # corresponding policies for this profile id\n",
    "    reduced_policies = [tuple(np.array(p)[mask]) for p in profile_to_policies[profile_id]]\n",
    "\n",
    "    # get losses and track lower bounds\n",
    "    pm = loss.compute_policy_means(Dk, yk, len(reduced_policies))\n",
    "    profile_lb = find_profile_lower_bound(Dk, yk, pm)\n",
    "    lower_bounds.append(profile_lb / N)\n",
    "    valid_pids.append(profile_id)\n",
    "\n",
    "lower_bounds = np.array(lower_bounds)\n",
    "best_loss = lower_bounds.min()\n",
    "print(f\"best_loss = {best_loss:.5f}\")\n",
    "lower_bounds = np.array(lower_bounds)\n",
    "total_lb = lower_bounds.sum()"
   ],
   "id": "6eec71b727a66623",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_loss = 0.25737\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T20:50:20.816268Z",
     "start_time": "2025-06-28T20:50:20.814387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# calculate rashomon threshold\n",
    "theta_global = total_lb * (1 + eps) # get loss threshold in absolute reference to sum of lower bounds\n",
    "if verbose:\n",
    "    print(f\"theta_global = {theta_global:.5f} from sum of lower bounds {total_lb:.5f}\")"
   ],
   "id": "beff4c16f6f47792",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_global = 26.17569 from sum of lower bounds 24.92923\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.3. Construct the RPS for each profile",
   "id": "9513341e613d6d4d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T20:50:21.126606Z",
     "start_time": "2025-06-28T20:50:21.103593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "R_profiles = []\n",
    "nonempty_profile_ids = []\n",
    "for i, profile_id in enumerate(valid_pids):\n",
    "    profile_mask = np.array(profiles[profile_id], dtype=bool)\n",
    "    M_k = profile_mask.sum()\n",
    "\n",
    "    # Compute reduced policies using only active features for this profile\n",
    "    reduced_policies = [tuple(np.array(p)[profile_mask]) for p in profile_to_policies[profile_id]]\n",
    "\n",
    "    # Compute number of levels for each local (profile) feature\n",
    "    R_k = np.array([len(set([p[feat] for p in reduced_policies])) for feat in range(M_k)])\n",
    "\n",
    "    # Remap each feature in reduced_policies to contiguous 0-based values\n",
    "    reduced_policies_arr = np.array(reduced_policies)\n",
    "    for j in range(reduced_policies_arr.shape[1]):\n",
    "        _, reduced_policies_arr[:, j] = np.unique(reduced_policies_arr[:, j], return_inverse=True)\n",
    "    reduced_policies = [tuple(row) for row in reduced_policies_arr]\n",
    "    R_k = np.array([len(np.unique(reduced_policies_arr[:, j])) for j in range(M_k)])\n",
    "\n",
    "    # Value-mapping-based subsetting and remapping\n",
    "    # This gives Dk (local indices) and yk (outcomes)\n",
    "    policy_indices_this_profile = profile_to_indices[profile_id]\n",
    "    mask = np.isin(D1, policy_indices_this_profile)\n",
    "    Dk = D1[mask]\n",
    "    yk = y1[mask]\n",
    "\n",
    "    # Now remap Dk from global policy indices to local indices in reduced_policies\n",
    "    Dk = np.asarray(Dk).reshape(-1)\n",
    "    policy_map = {idx: j for j, idx in enumerate(policy_indices_this_profile)}\n",
    "    assert all(ix in policy_map for ix in Dk), f\"Found Dk values not in mapping for profile {profile_id}\"\n",
    "    Dk_local = np.vectorize(policy_map.get)(Dk)      # map to local indices, shape (n,)\n",
    "    assert yk.shape[0] == Dk_local.shape[0], \"Dk_local and yk must have the same length\"\n",
    "\n",
    "    # Need to have Dk as a 1D array for the loss functions\n",
    "    # Compute policy means with local indices\n",
    "    pm = loss.compute_policy_means(Dk_local, yk, len(reduced_policies))\n",
    "    assert pm.shape[0] == len(reduced_policies), \"policy_means length mismatch\"\n",
    "\n",
    "    # get profile threshold\n",
    "    theta_k = max(0.0, theta_global - (total_lb - lower_bounds[i]))\n",
    "\n",
    "    # Need to reshape np array because the RAggregate_profile expects shape (n,1)\n",
    "    Dk_local = Dk_local.reshape(-1, 1)\n",
    "    yk = yk.reshape(-1, 1)\n",
    "    # get rashomon set for each profile\n",
    "    rashomon_profile = RAggregate_profile(\n",
    "        M=M_k,\n",
    "        R=R_k,\n",
    "        H=H,\n",
    "        D=Dk_local,  # Already local indices\n",
    "        y=yk,\n",
    "        theta=theta_k,\n",
    "        profile=tuple(profiles[profile_id]),\n",
    "        reg=lambda_r,\n",
    "        policies=reduced_policies,\n",
    "        policy_means=pm,\n",
    "        normalize=N\n",
    "    )\n",
    "\n",
    "    # calculate losses for non-empty profiles and add to list of profiles\n",
    "    Dk = np.asarray(Dk).reshape(-1) # loss functions again want a 1d array for D, but keep y 2d\n",
    "    if len(rashomon_profile) > 0:\n",
    "        rashomon_profile.calculate_loss(Dk_local, yk, reduced_policies, pm, lambda_r, normalize=N)\n",
    "        R_profiles.append(rashomon_profile)\n",
    "        nonempty_profile_ids.append(profile_id)\n",
    "    if verbose:\n",
    "        print(f\"Profile {profile_id}: M_k={M_k}, #policies={len(reduced_policies)}, theta_k={theta_k:.5f}, RPS size={len(rashomon_profile)}\")"
   ],
   "id": "18ba919ee77399d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile 1: M_k=1, #policies=3, theta_k=2.63517, RPS size=2\n",
      "Profile 2: M_k=1, #policies=2, theta_k=1.69617, RPS size=1\n",
      "Profile 3: M_k=2, #policies=6, theta_k=2.62537, RPS size=2\n",
      "Profile 4: M_k=1, #policies=2, theta_k=1.50383, RPS size=1\n",
      "Profile 5: M_k=2, #policies=6, theta_k=3.00661, RPS size=2\n",
      "Profile 6: M_k=2, #policies=4, theta_k=2.72556, RPS size=1\n",
      "Profile 7: M_k=3, #policies=12, theta_k=5.52833, RPS size=2\n",
      "Profile 9: M_k=2, #policies=3, theta_k=2.12428, RPS size=2\n",
      "Profile 10: M_k=2, #policies=2, theta_k=1.65157, RPS size=1\n",
      "Profile 11: M_k=3, #policies=6, theta_k=3.34851, RPS size=2\n",
      "Profile 12: M_k=2, #policies=2, theta_k=1.51779, RPS size=1\n",
      "Profile 13: M_k=3, #policies=6, theta_k=3.59946, RPS size=2\n",
      "Profile 14: M_k=3, #policies=4, theta_k=3.03184, RPS size=1\n",
      "Profile 15: M_k=4, #policies=12, theta_k=7.38522, RPS size=2\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T20:50:21.239031Z",
     "start_time": "2025-06-28T20:50:21.236832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# if none of the profiles have a rashomon set, observed RPS is empty\n",
    "if len(R_profiles) == 0:\n",
    "    if verbose:\n",
    "        print(\"No profiles have feasible Rashomon sets; global RPS is empty.\")"
   ],
   "id": "aebb5f736b7a1180",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T20:50:21.373273Z",
     "start_time": "2025-06-28T20:50:21.370927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "excluded_profiles = [profile_id for profile_id in valid_pids if profile_id not in nonempty_profile_ids]\n",
    "if verbose:\n",
    "    if len(excluded_profiles) > 0:\n",
    "        print(f\"Skipped profile number due to empty Rashomon set: {excluded_profiles}\")\n",
    "    else:\n",
    "        print(\"All profiles with data have non-empty Rashomon sets.\")"
   ],
   "id": "f3545a0db14eaccc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All profiles with data have non-empty Rashomon sets.\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T20:50:21.620583Z",
     "start_time": "2025-06-28T20:50:21.617393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Visual check on losses\n",
    "for idx, rp in enumerate(R_profiles):\n",
    "    losses = np.array(rp.loss)\n",
    "    print(f\"Profile {nonempty_profile_ids[idx]}: min loss = {losses.min():.4f}, max loss = {losses.max():.4f}\")"
   ],
   "id": "fd0a5c87c5b0480b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile 1: min loss = 1.4170, max loss = 1.4187\n",
      "Profile 2: min loss = 0.4697, max loss = 0.4697\n",
      "Profile 3: min loss = 1.4389, max loss = 1.4742\n",
      "Profile 4: min loss = 0.2774, max loss = 0.2774\n",
      "Profile 5: min loss = 1.8201, max loss = 2.0287\n",
      "Profile 6: min loss = 1.5191, max loss = 1.5191\n",
      "Profile 7: min loss = 4.4019, max loss = 5.2520\n",
      "Profile 9: min loss = 0.8927, max loss = 0.9078\n",
      "Profile 10: min loss = 0.4251, max loss = 0.4251\n",
      "Profile 11: min loss = 2.1309, max loss = 2.1621\n",
      "Profile 12: min loss = 0.2913, max loss = 0.2913\n",
      "Profile 13: min loss = 2.4130, max loss = 3.0275\n",
      "Profile 14: min loss = 1.8254, max loss = 1.8254\n",
      "Profile 15: min loss = 6.2588, max loss = 6.4168\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.4. Construct full RPS across all observed profiles",
   "id": "880994568701a873"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T20:50:21.892370Z",
     "start_time": "2025-06-28T20:50:21.886704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assemble observed RPS via find_feasible_combinations from RAggregate\n",
    "from rashomon.aggregate import find_feasible_combinations\n",
    "R_set = find_feasible_combinations(R_profiles, theta_global, H)\n",
    "if verbose:\n",
    "    print(f\"RPS has: {len(R_set)} feasible partitions over {len(R_profiles)} observed profiles.\")"
   ],
   "id": "863353d88dea3fe5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RPS has: 57 feasible partitions over 14 observed profiles.\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We note that directly constructing the RPS from the original Rashomon module does not work here. In particular, not all profiles necessarily have observations, and the original module return an empty RPS in that case. In particular, we need to correctly subset and mask the profiles, compute loss, and track indices, each of which have slightly different assumptions in the original module.",
   "id": "550fb5a98dfcbeb8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Second-wave allocation",
   "id": "e216bc08a51b126c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### (5.0.) Quick validity checks from first-wave",
   "id": "56eb57fd15349844"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T20:50:22.715320Z",
     "start_time": "2025-06-28T20:50:22.707319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Number of profiles: {len(profiles)}\")\n",
    "print(f\"Number of R_profiles (number of profiles with data): {len(R_profiles)}\")\n",
    "print(f\"Final RPS size (len(R_set)): {len(R_set)}\")\n",
    "\n",
    "assert len(D1) == len(y1), \"Mismatch: D and y must have same length.\"\n",
    "print(\"CHECKED: Assignment and outcome vectors are same length.\")\n",
    "\n",
    "K, M = np.array(policies).shape\n",
    "assert K > 0 and M > 0, \"Policies array shape invalid.\"\n",
    "print(f\"CHECKED: Lattice has {K} policies of {M} features each.\")\n",
    "\n",
    "assert len(R_profiles) > 0, \"No nonempty profiles in R_profiles!\"\n",
    "assert len(R_set) > 0, \"RPS is empty! No feasible partitions found.\"\n",
    "print(\"CHECKED: All observed profiles are nonempty, and RPS is nonempty.\")\n",
    "\n",
    "for pid, indices in profile_to_indices.items():\n",
    "    assert all(0 <= ix < K for ix in indices), f\"Profile {pid} has out-of-bounds policy indices.\"\n",
    "print(\"CHECKED: All profile_to_indices entries are valid global policy indices.\")\n",
    "\n",
    "for idx, rp in enumerate(R_profiles):\n",
    "    assert hasattr(rp, 'sigma'), f\"R_profile {idx} missing 'sigma'.\"\n",
    "    assert hasattr(rp, 'loss'), f\"R_profile {idx} missing 'loss'.\"\n",
    "    assert len(rp.sigma) == len(rp.loss), f\"R_profile {idx}: sigma/loss length mismatch.\"\n",
    "    assert np.all(np.isfinite(rp.loss)), f\"R_profile {idx}: loss contains NaN or inf.\"\n",
    "print(\"CHECKED: All R_profiles have matching, finite loss and partition arrays.\")\n",
    "\n",
    "if hasattr(R_profiles[0], 'profile'):\n",
    "    for rp in R_profiles:\n",
    "        pid = rp.profile if hasattr(rp, 'profile') else None\n",
    "        if pid is not None:\n",
    "            assert pid in profile_to_indices, f\"R_profile profile id {pid} not in profile_to_indices.\"\n",
    "print(\"CHECKED: All R_profiles have valid profile IDs.\")\n",
    "\n",
    "#--- RPS partition indices in range ---\n",
    "for partition in R_set:\n",
    "    assert all(0 <= idx < len(R_profiles[k].sigma) for k, idx in enumerate(partition)), \\\n",
    "        f\"Partition {partition} has out-of-range index.\"\n",
    "print(\"CHECKED: All RPS partitions refer to valid indices in R_profiles.\")"
   ],
   "id": "3542cb980c3cfd85",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of profiles: 16\n",
      "Number of R_profiles (number of profiles with data): 14\n",
      "Final RPS size (len(R_set)): 57\n",
      "CHECKED: Assignment and outcome vectors are same length.\n",
      "CHECKED: Lattice has 72 policies of 4 features each.\n",
      "CHECKED: All observed profiles are nonempty, and RPS is nonempty.\n",
      "CHECKED: All profile_to_indices entries are valid global policy indices.\n",
      "CHECKED: All R_profiles have matching, finite loss and partition arrays.\n",
      "CHECKED: All R_profiles have valid profile IDs.\n",
      "CHECKED: All RPS partitions refer to valid indices in R_profiles.\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.1. Next wave allocation weights",
   "id": "2021f8892e2fd44c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T20:50:23.253224Z",
     "start_time": "2025-06-28T20:50:23.249970Z"
    }
   },
   "cell_type": "code",
   "source": "from rashomon.extract_pools import lattice_edges, aggregate_pools, extract_pools",
   "id": "fb7562d9d0fbd867",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T20:50:23.513565Z",
     "start_time": "2025-06-28T20:50:23.495818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# precompute all lattice edges\n",
    "lattice_ed = lattice_edges(policies)  # policies holds the full enumerated lattice\n",
    "\n",
    "K = len(policies)\n",
    "boundary_counts = np.zeros(K, float)\n",
    "\n",
    "# compute posterior weights for each RPS partition\n",
    "Q = np.array([\n",
    "    sum(R_profiles[k].loss[part[k]] for k in range(len(R_profiles)))\n",
    "    for part in R_set\n",
    "])\n",
    "post_weights = np.exp(-Q - Q.min())\n",
    "post_weights /= post_weights.sum()"
   ],
   "id": "76354b97999ecc77",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T20:50:23.773692Z",
     "start_time": "2025-06-28T20:50:23.691118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for part, w_i in zip(R_set, post_weights):\n",
    "    pi_policies_profiles = {}\n",
    "    for k, rp in enumerate(R_profiles):\n",
    "        profile_id = nonempty_profile_ids[k]\n",
    "        profile_mask = np.array(profiles[profile_id], dtype=bool)\n",
    "\n",
    "        # Mask and remap as in RPS construction\n",
    "        local_policies = [tuple(np.array(p)[profile_mask]) for p in profile_to_policies[profile_id]]\n",
    "        if len(local_policies) == 0:\n",
    "            continue  # skip empty\n",
    "        arr = np.array(local_policies)\n",
    "        for j in range(arr.shape[1]):\n",
    "            _, arr[:, j] = np.unique(arr[:, j], return_inverse=True)\n",
    "        local_policies_remap = [tuple(row) for row in arr]\n",
    "\n",
    "        # Use the same remapped local_policies as when RPS was constructed\n",
    "        sigma = rp.sigma[part[k]]\n",
    "        _, pi_policies_local = extract_pools(local_policies_remap, sigma)\n",
    "\n",
    "        # Map from local index to global index\n",
    "        for local_idx, global_idx in enumerate(profile_to_indices[profile_id]):\n",
    "            pi_policies_profiles[global_idx] = pi_policies_local[local_idx]\n",
    "\n",
    "    # Build full K-vector: -1 for not-in-any-profile\n",
    "    pi_policies = np.full(K, -1, dtype=int)\n",
    "    for global_idx, pool_id in pi_policies_profiles.items():\n",
    "        pi_policies[global_idx] = pool_id\n",
    "\n",
    "    # Only count boundaries between *observed* nodes\n",
    "    for u, v in lattice_ed:\n",
    "        if pi_policies[u] != -1 and pi_policies[v] != -1 and pi_policies[u] != pi_policies[v]:\n",
    "            boundary_counts[u] += w_i\n",
    "            boundary_counts[v] += w_i\n",
    "\n",
    "if boundary_counts.sum() == 0:\n",
    "    raise ValueError(\"No boundaries detected in second-wave allocation. Check RPS content.\")\n",
    "\n",
    "boundary_probs_2 = boundary_counts / boundary_counts.sum()\n",
    "print(f\"Second-wave boundary allocation probabilities sum to {boundary_probs_2.sum():.4f}\")\n",
    "print(f\"Nonzero probabilities: {np.count_nonzero(boundary_probs_2)} of {K} policies.\")"
   ],
   "id": "2332e3d683d833c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second-wave boundary allocation probabilities sum to 1.0000\n",
      "Nonzero probabilities: 70 of 72 policies.\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.2. Allocate and simulate second-wave outcomes",
   "id": "d9c18734529360f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We now use the modular versions for concision. See first section for exact code.",
   "id": "6ed2890b1cf25cf6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T20:50:25.362563Z",
     "start_time": "2025-06-28T20:50:25.357984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n2 = 500\n",
    "\n",
    "# allocate nodes to each policy and get D2\n",
    "alloc2 = allocation.allocate_wave(boundary_probs_2, n2)\n",
    "assert alloc2.sum() == n2\n",
    "\n",
    "D2 = allocation.assign_treatments(alloc2)"
   ],
   "id": "32e7503cc1049c5",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We then simulate outcomes, again using our true outcomes.",
   "id": "2aa847ebd0f1db1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T20:50:25.874435Z",
     "start_time": "2025-06-28T20:50:25.871793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# still the same beta for the underlying causal function\n",
    "y2 = generate_outcomes(D=D2, beta=beta, sigma_noise=sigma_noise, random_seed=55)"
   ],
   "id": "acd28a52bb73fac2",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.3. Construct final RPS",
   "id": "391f58a6d2074de3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We now combine the outcomes from the first and second waves to construct the final RPS.",
   "id": "672eeac38e0db035"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T20:50:27.003064Z",
     "start_time": "2025-06-28T20:50:26.998341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "D_full = np.vstack([D1, D2])\n",
    "y_full = np.concatenate([y1, y2])"
   ],
   "id": "8951a1de8780025b",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ... see two_wave_sim.ipynb for final construction of RPS, now using a modular approach",
   "id": "92a22682dd242649"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
