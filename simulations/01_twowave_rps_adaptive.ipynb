{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# SIM 3.0: Adaptive RPS algorithm, two-wave allocation\n",
   "id": "edf66d8b59319957"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import visualizations\n",
    "from rashomon.hasse import policy_to_profile, enumerate_policies, enumerate_profiles\n",
    "from rashomon.aggregate import RAggregate\n",
    "from datagen import (\n",
    "    phi_basic, phi_linear_interact, phi_grouped_smooth, phi_grouped_coarse,\n",
    "    phi_grouped_smooth2, phi_peak, generate_data_from_assignments\n",
    ")\n",
    "from allocation import (\n",
    "    compute_policy_variances,\n",
    "    allocate_wave1,\n",
    "    allocate_wave2,\n",
    "    allocate_wave2_pools,\n",
    "    create_assignments_from_alloc\n",
    ")\n",
    "from rashomon.extract_pools import extract_pools, aggregate_pools\n",
    "from rashomon.loss import compute_pool_means, compute_policy_means\n",
    "from rashomon.metrics import make_predictions\n"
   ],
   "id": "f9d4a617b441d662",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# === Config ===\n",
    "M = 3\n",
    "R = np.array([5, 5, 3])\n",
    "lambda_reg = 0.01                  # regularization parameter\n",
    "epsilon = 0.05                      # tolerance off MAP (used by visualizations if needed)\n",
    "H = 100\n",
    "\n",
    "allocation_rule_wave1 = \"minimax\"\n",
    "allocation_rule_wave2 = \"minimax\"\n",
    "within_pool_rule = \"minimax\"\n",
    "\n",
    "max_alloc = 300\n",
    "feasible_waves = 4                  # two-wave demo\n",
    "sig = 0.2\n",
    "\n",
    "verbose = True\n",
    "top_k = 10\n",
    "n_preview = 30\n",
    "num_workers = 2\n",
    "\n",
    "# Sweep parameter for RPS size tuning (coarse-to-fine)\n",
    "theta_init = 0.1\n",
    "theta_init_step = 0.05\n",
    "min_rset_size = 100\n"
   ],
   "id": "88636c2abab4ee20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# === Enumerate policies and profiles ===\n",
    "all_policies = enumerate_policies(M, R)\n",
    "num_policies = len(all_policies)\n",
    "profiles, profile_map = enumerate_profiles(M)\n",
    "num_profiles = len(profiles)\n",
    "\n",
    "# profile index mappings\n",
    "policies_profiles = {}\n",
    "policies_ids_profiles = {}\n",
    "policies_profiles_masked = {}  # masked policies hold the active features only\n",
    "for k, profile in enumerate(profiles):\n",
    "    ids = [i for i, p in enumerate(all_policies) if policy_to_profile(p) == profile]\n",
    "    policies_ids_profiles[k] = ids\n",
    "    policies_profiles[k] = [all_policies[i] for i in ids]\n",
    "\n",
    "    profile_mask = [bool(v) for v in profile]  # t/f map of which features are active\n",
    "    masked_policies = [tuple([pol[i] for i in range(M) if profile_mask[i]]) for pol in policies_profiles[k]]\n",
    "    policies_profiles_masked[k] = masked_policies\n"
   ],
   "id": "e885689c2a968788",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# === Wave budgets ===\n",
    "# Wave 1 must be >= #policies (enforce one-per-policy coverage via allocate_wave1 floors)\n",
    "n = [num_policies]\n",
    "remaining_alloc = max_alloc - num_policies\n",
    "if remaining_alloc < 1:\n",
    "    raise Exception(f\"Need at least {num_policies} observations for one-per-policy coverage in Wave 1.\")\n",
    "if H < num_profiles:\n",
    "    raise Exception(f\"Need H ≥ #profiles ({num_profiles}) for initial per-profile pooling space.\")\n",
    "adaptive_waves = feasible_waves - 1\n",
    "\n",
    "# Allocate remaining equally across later waves\n",
    "if adaptive_waves > 0:\n",
    "    base = remaining_alloc // adaptive_waves\n",
    "    remainder = remaining_alloc % adaptive_waves\n",
    "    n.extend([base + 1 if i < remainder else base for i in range(adaptive_waves)])\n",
    "if verbose:\n",
    "    print(f\"Per-wave allocation: {n}, total={sum(n)}\")\n"
   ],
   "id": "3edaa5558d128cd8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# === Oracle outcomes (ground truth generator) ===\n",
    "oracle_outcomes = np.array([phi_grouped_coarse(p, R) for p in all_policies])\n",
    "\n",
    "oracle_rank_to_policy = np.argsort(-oracle_outcomes)           # index 0 gives best policy index\n",
    "oracle_policy_to_rank = np.empty_like(oracle_rank_to_policy)\n",
    "oracle_policy_to_rank[oracle_rank_to_policy] = np.arange(len(oracle_outcomes))\n",
    "\n",
    "top_k_indices = oracle_rank_to_policy[:top_k]\n",
    "top_k_policies = [all_policies[i] for i in top_k_indices]\n",
    "top_k_values = oracle_outcomes[top_k_indices]\n",
    "\n",
    "if verbose:\n",
    "    print(\"Top-k best policies and their profiles:\")\n",
    "    for rank, idx in enumerate(top_k_indices, 1):\n",
    "        policy = all_policies[idx]\n",
    "        profile = policy_to_profile(policy)\n",
    "        print(f\"Rank {rank}: Policy idx {idx}, Policy {[int(i) for i in policy]}, Profile {profile}\")\n"
   ],
   "id": "88e6c4ecbf1e132e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# === Containers ===\n",
    "metrics_per_wave = []\n",
    "D = np.empty((0, 1), dtype=int)\n",
    "y = np.empty((0, 1), dtype=float)\n",
    "pools_for_next_wave = None   # will be filled after Wave-1 RPS\n"
   ],
   "id": "dddc7da269cd7bc6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# === Wave loop ===\n",
    "for wave_number in range(1, feasible_waves + 1):\n",
    "    np.random.seed(wave_number)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Decide allocation D_wave\n",
    "    # ---------------------------\n",
    "    if wave_number == 1:\n",
    "        # Policy-level design with coverage+variance-inflation logic\n",
    "        policy_variances, policy_counts = compute_policy_variances(D, y, num_policies)   # empty -> all counts 0\n",
    "        sigmas = np.sqrt(policy_variances)\n",
    "\n",
    "        n_wave = n[0]\n",
    "        alloc = allocate_wave1(\n",
    "            allocation_rule_wave1,     # \"neyman_policy\" | \"minimax_policy\" | \"best_arm\"\n",
    "            sigmas=sigmas,\n",
    "            counts=policy_counts,\n",
    "            N=n_wave\n",
    "        )\n",
    "        D_wave = create_assignments_from_alloc(alloc).reshape(-1, 1)\n",
    "    else:\n",
    "        # Pool→Policy two-stage design using pools learned from previous wave (MAP)\n",
    "        if pools_for_next_wave is None:\n",
    "            raise RuntimeError(\"Wave-2 requires pools from Wave-1. Ensure Wave-1 RPS completed.\")\n",
    "        policy_variances, policy_counts = compute_policy_variances(D, y, num_policies)\n",
    "        policy_sigmas = np.sqrt(policy_variances)\n",
    "\n",
    "        n_wave = n[wave_number - 1]\n",
    "        alloc_policy = allocate_wave2_pools(\n",
    "            rule=allocation_rule_wave2,          # \"neyman_pool\" | \"minimax_pool\" | \"best_pool\"\n",
    "            pool_to_policies=pools_for_next_wave,\n",
    "            policy_sigmas=policy_sigmas,\n",
    "            N=n_wave,\n",
    "            policy_counts=policy_counts,\n",
    "            pool_weights=None,                   # equal weights in each pool\n",
    "            pool_gaps=None,                      # supply if using 'best_pool'\n",
    "            within_rule=within_pool_rule         # \"neyman\" / \"minimax\" / \"uniform\"\n",
    "        )\n",
    "        D_wave = create_assignments_from_alloc(alloc_policy).reshape(-1, 1)\n",
    "\n",
    "    # ---------------------------\n",
    "    # Generate outcomes this wave\n",
    "    # ---------------------------\n",
    "    X_wave, y_wave = generate_data_from_assignments(D_wave, all_policies, oracle_outcomes, sig=sig)\n",
    "\n",
    "    # Accumulate full data so far\n",
    "    D = np.vstack([D, D_wave])\n",
    "    y = np.vstack([y, y_wave])\n",
    "\n",
    "    # ---------------------------\n",
    "    # Policy stats (means/vars)\n",
    "    # ---------------------------\n",
    "    policy_stats = compute_policy_means(D, y, num_policies)\n",
    "    policy_variances, policy_counts = compute_policy_variances(D, y, num_policies)\n",
    "    policy_sigmas = np.sqrt(policy_variances)\n",
    "    # ---------------------------\n",
    "    # Enumerate Rashomon set for this wave (coarse-to-fine theta)\n",
    "    # ---------------------------\n",
    "    if wave_number == 1:\n",
    "        theta = theta_init\n",
    "        theta_step = theta_init_step\n",
    "\n",
    "    max_steps = 50\n",
    "    num_sweeps = 3\n",
    "    for sweep in range(num_sweeps):\n",
    "        steps = 0\n",
    "        while steps < max_steps:\n",
    "            if verbose:\n",
    "                print(f\"Trying theta: {theta:.4f}\")\n",
    "            R_set, R_profiles = RAggregate(\n",
    "                M, R, H, D, y, theta,\n",
    "                reg=lambda_reg, num_workers=num_workers, verbose=False\n",
    "            )\n",
    "            if verbose:\n",
    "                print(f\"Theta: {theta:.4f} -- RPS size: {len(R_set)}\")\n",
    "\n",
    "            # coarse-to-fine stepping around smallest theta with non-empty set\n",
    "            if len(R_set) > 0 and sweep == 0:\n",
    "                theta -= theta_step\n",
    "                theta_step = theta_step / (M*2)\n",
    "                break\n",
    "            if min_rset_size <= len(R_set) and sweep < num_sweeps - 1:\n",
    "                theta -= theta_step\n",
    "                theta_step = theta_step / (M*2)\n",
    "                break\n",
    "            if len(R_set) >= min_rset_size and sweep == num_sweeps - 1:\n",
    "                theta_step = theta_step * ((M*2)**(num_sweeps-1))\n",
    "                break\n",
    "\n",
    "            theta += theta_step\n",
    "            steps += 1\n",
    "\n",
    "    if len(R_set) == 0:\n",
    "        print(\"Warning: No feasible Rashomon set found within range.\")\n",
    "    elif verbose:\n",
    "        print(f\"End theta: {theta:.4f}, RPS size: {len(R_set)}\")\n",
    "        print(f\"Wave {wave_number} Rashomon set: {len(R_set)} feasible global partitions (combinations of per-profile poolings).\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # Posterior + metrics & cache pools for next wave\n",
    "    # ---------------------------\n",
    "    num_partitions = len(R_set)\n",
    "\n",
    "    regrets = []\n",
    "    best_pred_indices_all = []\n",
    "    policy_indices_all = []\n",
    "    policy_means_all = []\n",
    "\n",
    "    sorted_idx_all = []\n",
    "    sorted_means_all = []\n",
    "\n",
    "    partition_losses = np.zeros(num_partitions)\n",
    "\n",
    "    posterior_mse = []\n",
    "    posterior_best_mse = []\n",
    "    posterior_iou = []\n",
    "\n",
    "    pi_policies_r_list = []\n",
    "    pi_pools_r_list = []       # NEW: capture pools for MAP\n",
    "    pool_means_r_list = []\n",
    "\n",
    "    # track pools for each profile k, for each rashomon set r\n",
    "    for r, partition_r in enumerate(R_set):\n",
    "        pi_policies_profiles_r = {}\n",
    "\n",
    "        for k, profile in enumerate(profiles):\n",
    "            sigma_k = R_profiles[k].sigma[partition_r[k]]\n",
    "            if sigma_k is None:\n",
    "                # entire profile is a single pool\n",
    "                n_policies_profile = len(policies_profiles_masked[k])\n",
    "                pi_policies_r_k = {i: 0 for i in range(n_policies_profile)}\n",
    "            else:\n",
    "                _, pi_policies_r_k = extract_pools(policies_profiles_masked[k], sigma_k)\n",
    "\n",
    "            pi_policies_profiles_r[k] = pi_policies_r_k\n",
    "\n",
    "        # aggregate into global partition structures\n",
    "        pi_pools_r, pi_policies_r = aggregate_pools(pi_policies_profiles_r, policies_ids_profiles)\n",
    "        pool_means_r = compute_pool_means(policy_stats, pi_pools_r)\n",
    "\n",
    "        pi_pools_r_list.append(pi_pools_r)         # NEW\n",
    "        pi_policies_r_list.append(pi_policies_r)\n",
    "        pool_means_r_list.append(pool_means_r)\n",
    "\n",
    "        # Partition loss\n",
    "        partition_losses[r] = sum(R_profiles[k].loss[partition_r[k]] for k in range(len(partition_r)))\n",
    "\n",
    "        # Predictions by policy\n",
    "        policy_indices = np.array(list(pi_policies_r.keys()))\n",
    "        policy_means = np.array([pool_means_r[pi_policies_r[idx]] for idx in policy_indices])\n",
    "        order = np.argsort(-policy_means)\n",
    "        sorted_idx = policy_indices[order]\n",
    "        sorted_means = policy_means[order]\n",
    "\n",
    "        # Store results\n",
    "        policy_indices_all.append(policy_indices)\n",
    "        policy_means_all.append(policy_means)\n",
    "        sorted_idx_all.append(sorted_idx)\n",
    "        sorted_means_all.append(sorted_means)\n",
    "\n",
    "        best_pred = sorted_idx[0]\n",
    "        regret = float(oracle_outcomes[oracle_rank_to_policy[0]] - oracle_outcomes[best_pred])\n",
    "        regrets.append(regret)\n",
    "        best_pred_indices_all.append(best_pred)\n",
    "\n",
    "        # Posterior-weighted metrics\n",
    "        y_r_est = make_predictions(D, pi_policies_r, pool_means_r)\n",
    "\n",
    "        mse = np.mean((y_r_est - y) ** 2)  # mse on outcomes\n",
    "        best_mse = (oracle_outcomes[oracle_rank_to_policy[0]] - oracle_outcomes[best_pred]) ** 2\n",
    "        iou = len(set(sorted_idx[:top_k]) & set(top_k_indices)) / len(set(sorted_idx[:top_k]) | set(top_k_indices))\n",
    "\n",
    "        posterior_mse.append(mse)\n",
    "        posterior_best_mse.append(best_mse)\n",
    "        posterior_iou.append(iou)\n",
    "\n",
    "    # Posterior weights and expected metrics\n",
    "    posterior_weights = np.exp(-partition_losses)\n",
    "    posterior_weights /= posterior_weights.sum() if posterior_weights.sum() > 0 else 1.0\n",
    "    map_idx = int(np.argmin(partition_losses))\n",
    "    map_loss = float(partition_losses[map_idx])\n",
    "\n",
    "    policy_indices = policy_indices_all[map_idx]\n",
    "    policy_means = policy_means_all[map_idx]\n",
    "    order = np.argsort(-policy_means)\n",
    "\n",
    "    pi_policies_r = pi_policies_r_list[map_idx]\n",
    "    pi_pools_r = pi_pools_r_list[map_idx]\n",
    "    pool_means_r = pool_means_r_list[map_idx]\n",
    "\n",
    "    # pi_pools_r is a list-like structure where each entry is an array of global policy indices in that pool\n",
    "    pools_for_next_wave = {int(g): list(v) for g, v in pi_pools_r.items()}\n",
    "\n",
    "    sorted_idx = sorted_idx_all[map_idx]\n",
    "    sorted_means = sorted_means_all[map_idx]\n",
    "    oracle_values = oracle_outcomes[sorted_idx]\n",
    "    oracle_ranks = oracle_policy_to_rank[sorted_idx]\n",
    "    is_topk = [i in top_k_indices for i in sorted_idx]\n",
    "\n",
    "    expected_mse = float(np.dot(posterior_weights, posterior_mse))\n",
    "    expected_best_mse = float(np.dot(posterior_weights, posterior_best_mse))\n",
    "    expected_iou = float(np.dot(posterior_weights, posterior_iou))\n",
    "    expected_regret = float(np.dot(posterior_weights, regrets))\n",
    "\n",
    "    metrics_per_wave.append({\n",
    "        \"wave\": wave_number,\n",
    "        \"theta\": float(theta),\n",
    "        \"rps_size\": int(len(R_set)),\n",
    "        \"expected_regret\": expected_regret,\n",
    "        \"expected_mse\": expected_mse,\n",
    "        \"expected_best_mse\": expected_best_mse,\n",
    "        \"expected_iou\": expected_iou,\n",
    "        \"map_loss\": map_loss\n",
    "    })\n",
    "\n",
    "    if verbose:\n",
    "        # MAP Summary and regret plots --\n",
    "        df_map = visualizations.plot_map_true_vs_predicted_bar_topk(\n",
    "            sorted_idx=sorted_idx,\n",
    "            sorted_means=sorted_means,\n",
    "            oracle_beta=oracle_outcomes,\n",
    "            oracle_ranks=oracle_policy_to_rank,\n",
    "            top_k_indices=top_k_indices,\n",
    "            N=n_preview\n",
    "        )\n",
    "        visualizations.plot_map_regret_bar(sorted_idx, oracle_outcomes, oracle_rank_to_policy[0], N=n_preview)\n",
    "        visualizations.plot_map_regression(df_map)\n",
    "        visualizations.plot_oracle_ordered_bar(df_map, top_k_indices, oracle_outcomes, all_policies, N=n_preview)\n",
    "\n",
    "        df_map_obs = visualizations.plot_map_true_vs_predicted_bar_observed(\n",
    "            sorted_idx, sorted_means, oracle_outcomes, oracle_ranks,\n",
    "            np.where(policy_stats[:, 1] > 0)[0],   # fixed keyword\n",
    "            N=num_policies\n",
    "        )\n",
    "        visualizations.plot_oracle_ordered_bar(df_map_obs, top_k_indices, oracle_outcomes, all_policies, N=num_policies)\n",
    "        visualizations.plot_map_regression_observed(df_map_obs)\n",
    "\n",
    "        profile_losses = [rp.loss for rp in R_profiles]\n",
    "        visualizations.plot_minimax_risk_matrix(profile_losses, map_idx=map_idx)\n",
    "        print(f\"Number of possible partitions per profile: {[len(p) for p in profile_losses]}\")\n",
    "\n",
    "        # (A) Pool sizes\n",
    "        visualizations.plot_pool_size_bar(pools_for_next_wave)\n",
    "\n",
    "        # (B) Pool SE bars\n",
    "        visualizations.plot_pool_se_bar(pools_for_next_wave, policy_sigmas, policy_counts)\n",
    "\n",
    "        # (C) Pool mean vs SE proxy (use MAP pool means you already computed)\n",
    "        visualizations.plot_pool_mean_vs_se(pool_means_r, pools_for_next_wave, policy_sigmas, policy_counts)"
   ],
   "id": "8853239e4d0f89ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a737cae441f443f8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
